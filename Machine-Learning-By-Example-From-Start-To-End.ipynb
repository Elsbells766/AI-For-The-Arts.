{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0462a72",
   "metadata": {},
   "source": [
    "## In this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09dcad10",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'null' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m      1\u001b[39m {\n\u001b[32m      2\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mcells\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      3\u001b[39m   {\n\u001b[32m      4\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m97c259f2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m      7\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Machine Learning by Example: from Start to End\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m    ]\n\u001b[32m     10\u001b[39m   },\n\u001b[32m     11\u001b[39m   {\n\u001b[32m     12\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33md6539a1d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m     15\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Task 1: README!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#### The tasks in this notebook should be tackled in discussion with your peer group! \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m**By asking and answering each other questions, you learn much more than doing independent work.** The article [“Embracing Digitalization: Student Learning and New Technologies” (Crittenden, Biel & Lovely 2018)](https://journals.sagepub.com/doi/10.1177/0273475318820895) shows how we learn and retain more information when we explain and discuss it with others.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBefore you tackle a machine learning project you should make sure you keep the bigger picture in your head - this is your human input, skill, and imagination -  no AI can do this for you at the moment. The following sketches the typical steps involved in a machine learning project:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Step 1: Frame Your Problem\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - What is the task? - Who will use it in what environment? What are the risks and impact?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - How will you measure performance of your model? Measures sufficient to assess potential risks and impacts?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - What are the assumptions? Document and review assumptions for bias. Question everything.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Step 2: Get Your Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Download your data - How will your get your data from where? Permissions and licenses? Suitable and reliable?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Take a quick look at the data structure - how big is it? what fields/attributes are there and how many?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Set aside test data - random split or stratified split? \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Step 3: Prepare Your Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Handling Text/Categorical Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Scaling and Transformation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Separate the labels from the rest of the attributes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Step 4: Select and Train Your Model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - train and evaluate on the training set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - cross-validation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Step 6: Test on Completely New Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Step 7: Publish Your Results! Party! &#x1F389; &#x1F389; &#x1F389;\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn this notebook, we will go through some of the key the steps. Some tasks will involve critical reflection, and others will be about coding. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRemember that, **if you are taking more than 30 minutes to do one task without any progress**, you should probably take a break. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Note down what you did and what errors you got in a markdown cell. This will help you understand the recurring errors, you will understand where you left off when you come back to it later, and also help you when you discuss the problem with your peers and with the lab tutors.  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe code in this notebook is modified from that which was made available by Aurélien Géron and his fabulous book [\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mHands-On Machine Learning with Scitkit-Learn, Keras & Tensorflow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m](https://eleanor.lib.gla.ac.uk/record=b4094676).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m    ]\n\u001b[32m     49\u001b[39m   },\n\u001b[32m     50\u001b[39m   {\n\u001b[32m     51\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m6025a3c8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     53\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m     54\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 1-1: Checking Your Set Up\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     56\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIt is important not only to check that you have the correct set of software and packages, but also that the version is the right one. If versions are not compatible with your code then it will throw up errors or unexpected results.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     58\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Python\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     60\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCheck that your Python has version greater than 3.7 using the following code. This is what the code in the noteboook requires.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     62\u001b[39m    ]\n\u001b[32m     63\u001b[39m   },\n\u001b[32m     64\u001b[39m   {\n\u001b[32m     65\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mnull\u001b[49m,\n\u001b[32m     67\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33meb2a627b\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     68\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m     69\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     70\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport sys # importing the package sys which lets you talk to your computer system.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     73\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert sys.version_info >= (3, 7) #versions are expressed a pair of numbers (3, 7) which is equivalent to 3.7. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m    ]\n\u001b[32m     75\u001b[39m   },\n\u001b[32m     76\u001b[39m   {\n\u001b[32m     77\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2eb61a85\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     79\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m     80\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     81\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe `assert` statement throws up an error when the statement following it is not true. If it is true, nothing will be shown. Experiment by replacing the numbers in the round brackets to be much bigger. **A Pair of numbers** like `(3, 7)` in round brackets is a data structure known as a **tuple** in programming lingo. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     82\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     83\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Scikit-Learn\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     84\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     85\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFor this part you will need to have your environment installed with Python libraries `scikit-learn` and `packaging`. Review your Codespace exercise to rememebr how to install Python libraries. Note in the code below that when importing scikit-learn in the python code, you use `sklearn`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     86\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCheck that your Scikit-Learn package version is greater than 1.0.1. In this case you will need to import `version` which is part of the `packaging` Python library. This allows you to extract/parse version numbers for Python packages/libraries like `sklearn`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m    ]\n\u001b[32m     89\u001b[39m   },\n\u001b[32m     90\u001b[39m   {\n\u001b[32m     91\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     92\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m     93\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mc5882a1a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     94\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m     95\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     96\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     97\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom packaging import version #import the package \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mversion\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport sklearn # import scikit-learn\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    100\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33massert version.parse(sklearn.__version__) >= version.parse(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m1.0.1\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m    ]\n\u001b[32m    102\u001b[39m   },\n\u001b[32m    103\u001b[39m   {\n\u001b[32m    104\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    105\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2dafd09c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    107\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    108\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 1-2: Review Machine Learning\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    110\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 1: Create a markdown cell to demonstrate your own reflection\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    111\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    112\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- In your markdown cell embed an image or link to a diagram illustrating the workflow from data to algorithm to model and data to model to predicted output. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - To embed images in your markdown cell, you can use the syntax `![alt text](image.jpg)` where you replace alt text with a description of your image (keep the square brackets!) and replace image.jpg with the file path and name of your image. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - To include a URL, use the syntax `[title](https://www.example.com)` where you replace title  with your own description, and https://www.example.com  with your own URL. Keep all brackets intact. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - For your reference, you can refer to the [markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/)  - note that HTML codes are also understood by your notebook.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Explain in your markdown cell how the examples in Lectures 3 & 4 align with the workflow. For example, what is the data, what was the learning algorithm, what is the model and what did the model output in response to new data?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- In  your markdown cell, reflect on the range of ways to explain the workflow and the examples to a wider audience, for example, a museum curator?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    119\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve created a cell for you to use already below - double click on the area to start editing.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m    ]\n\u001b[32m    121\u001b[39m   },\n\u001b[32m    122\u001b[39m   {\n\u001b[32m    123\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m72fdf563\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    125\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    126\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    127\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m***Markdown cell to modify***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    128\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    129\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    130\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    131\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m3. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    132\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m4.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m    ]\n\u001b[32m    134\u001b[39m   },\n\u001b[32m    135\u001b[39m   {\n\u001b[32m    136\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    137\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33md522ddc0-38c2-4eea-8dc5-227f9e6eee4a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    138\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    139\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    140\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 2: Discuss and report your reflection with your group\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    141\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Get together with your peer group. Take turns to discuss your reflection above. If you have any difficulties, discuss these also.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    142\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Note down the results of your discussion in your notebook. In particular, note down anything that help you or others learn the topic. What approach could take in your notebook to engage the wider audience with your machine learning code.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    143\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    144\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve created a cell for you to use already below - double click on the area to start editing.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    145\u001b[39m    ]\n\u001b[32m    146\u001b[39m   },\n\u001b[32m    147\u001b[39m   {\n\u001b[32m    148\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    149\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m784b8573-0e7e-478a-a9b4-726386dc8b37\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    151\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    152\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m***Markdown cell to modify***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    153\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    154\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    155\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    156\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m3. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    157\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m4.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m    ]\n\u001b[32m    159\u001b[39m   },\n\u001b[32m    160\u001b[39m   {\n\u001b[32m    161\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    162\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m027e97d4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    163\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    164\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    165\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 1-3: Framing the Problem\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    166\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    167\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn this notebook, we will be working with two datasets:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    168\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    169\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1)\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mTabular data consisting of information about houses in districts within the US state of California, and, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    170\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2)\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mImage pixel data, each image representing a digit handwritten by high school students and employees of the US Census Bureau. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    171\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    172\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe first of these datasets will be used to **predict median housing prices for a given district**. The results of the prediction will be combined with other data to determine whether it is worth investing in a given district. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    173\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    174\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe second of these datasets will be used to **classify hand written digits**. It was originally developed as a way of sorting out the handwritten US zip codes (similar to UK postcodes) at the post office. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    175\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    176\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    177\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 1: Understand how framing the problem affects data selection\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    178\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    179\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- The academic article [“Rethinking the field of automatic prediction of court decisions”](https://link.springer.com/article/10.1007/s10506-021-09306-3) by Medvedeva, Wieling & Vols (2023), to appreciate how, depending on the objectives, the characteristics of data and algorithm might differ. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    180\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Read the BBC article [“AI facial recognition: Campaigners and MPs call for ban”](https://www.bbc.co.uk/news/technology-67022005) to understand that the same data, depending on its use, can raise concern about AI. We will be discussing prediction court decisions further in Week 7.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    181\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- In view of the above, write down your reflection on the importance of framing your problem precisely in a notebook markdown cell - not only to define the task properly, but to understand how your machine learning model will be used down the road. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    182\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    183\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 2: How to select your algorithm\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    184\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    185\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn Lecture 2, we discussed how machine learning can be divided into three types: Supervised, Unsupervised, and Reinforcement. Large part of this course is focused on supervised learning – in particular, in this notebook, we will explore this using examples of regression and classification.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    186\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    187\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m**To refresh your memory, read this short article from Codecademy** – [“Regression vs Classification”](https://www.codecademy.com/article/regression-vs-classification).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    188\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    189\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Discuss with your peer group whether regression or classification would fit better for predicting median housing prices.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    190\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Discuss with your peer group whether regression or classification would fit better for handwritten digit recognition.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    191\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Write down the results of the discussion. In particular, report on what you concluded after the discussion and why.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    192\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    193\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 3: Before Data Collection\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    194\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    195\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOnce your problem is defined (e.g. predicting the median housing price of a district), you will need to collect a new data set appropriate for your task, and/or identify existing data sets that can be used for training your model. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    196\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    197\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Discuss with your peer group what kind of information about housing in a district you think would help predict the median housing price in the district.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    198\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Discuss how these decisions might depend on geographical and/or cultural differences and how the information you collect would already bias the data. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    199\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    200\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m**Note these down the results of the discussions in Steps 2 & 3 in a markdown cell below.** I have already created a markdown cell for your use - just double click the area to begin editing.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m    ]\n\u001b[32m    202\u001b[39m   },\n\u001b[32m    203\u001b[39m   {\n\u001b[32m    204\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    205\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m0e3715aa\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    206\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    207\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    208\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m***Markdown cell to modify for Steps 1, 2 and 3***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    210\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    211\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    212\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    213\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    214\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m3.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    216\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m4.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    217\u001b[39m    ]\n\u001b[32m    218\u001b[39m   },\n\u001b[32m    219\u001b[39m   {\n\u001b[32m    220\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m4c9ae4ec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    223\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    224\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Working with Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOverviews of machine learning and AI often make it seem as though the largest part of machine learning is in training the algorithm. This is misleading. In fact, [Forbes reported that about 80\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[33mf data science is related to data preparation](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/). This includes, among other things, data cleaning, re-scaling, and labelling.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    228\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIf you also include things like keeping track of what you did and why, storing and backing up data generated as well as the data used at the beginning, and recording the evaluation results, data exploration, interpretation, I would say that **95\u001b[39m\u001b[33m%\u001b[39m\u001b[33m** of machine learning is involved in **data management**. This can, in fact, be said to be a substantial part of achieving transparency, a corner stone of addressing the ethical concerns regarding AI and bias, fairness, data protection, explainability etc.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    229\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    230\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSo, let\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms get some data!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    231\u001b[39m    ]\n\u001b[32m    232\u001b[39m   },\n\u001b[32m    233\u001b[39m   {\n\u001b[32m    234\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    235\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mc15b4f64\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    236\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    237\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    238\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Task 2: Getting Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    239\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    240\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBefore anything else, you must get the data! There many ways you can get data. The scikit-learn\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms `datasets` package has some datasets already available to you. You can also get data from a nuumber of places such as OECD, OpenML, Kaggle, individual repositories in GitHub. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    241\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    242\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn reality, data is everywhere. In fact, artists who make use of AI often make their own datasets: for example, check out [Anna Ridler\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms shell images](https://annaridler.com/the-shell-record-2021), [Caroline Sinders\u001b[39m\u001b[33m'\u001b[39m\u001b[33m feminist dataset](https://carolinesinders.com/feminist-data-set/), and [Refik Anadol\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms coral images](https://refikanadol.com/works-old/artificial-realities-coral/). While these may be owned by the artists, it can inspire new ways of thinking about data.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    243\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    244\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn this task we will look at something a little less artistic! &#x1F609;\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    245\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    246\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- **Example Tabular Data**: dataset comprising housing prices in California in the the United States. This dataset is available on the GitHub, courtesy of Aurelien Geron. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    247\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- **Example Image Data**: MNIST dataset comprising images of handwritten digits. Handwritten digit recognition with the MNIST dataset is sometimes called the **\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mHello World!\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m of machine learning**. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    248\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    249\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWe will use these datasets to carry out the prediction of housing prices and classification of digit images.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m    ]\n\u001b[32m    251\u001b[39m   },\n\u001b[32m    252\u001b[39m   {\n\u001b[32m    253\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    254\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m656eba8c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    255\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    256\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    257\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 2-1: Download the Data: Example Tabular Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    258\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    259\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe following code defines **function** called `load_housing_data()`. This function retrieves a compressed file avaialable at `https://github.com/ageron/data/raw/main/housing.tgz` and saves it in a folder `datasets` which is in the same folder as this notebook. This will be created if it does not exist. The code will also extract the contents in the folder `datasets`. It will then return the content of the data file `housing.csv` in the folder as a Pandas dataframe.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    260\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    261\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPandas is a powerful and popular open-source data analysis and manipulation library for the Python programming language. It provides data structures and functions needed to work with structured data, such as tabulated data, seamlessly. Here are some key features:\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    262\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- DataFrame: A 2-dimensional labeled data structure with columns of potentially different types, similar to a table in a database or an Excel spreadsheet.\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    264\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Series: A 1-dimensional labeled array capable of holding any data type.\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    265\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Data Cleaning: Tools for handling missing data, filtering, and transforming data.\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    266\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Data Wrangling: Functions for merging, joining, and reshaping datasets.\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    267\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Time Series: Capabilities for working with time-series data, including date range generation and frequency conversion.\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    268\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    269\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPandas is widely used in data science, machine learning, and data analysis projects due to its ease of use and flexibility. ing it?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    270\u001b[39m    ]\n\u001b[32m    271\u001b[39m   },\n\u001b[32m    272\u001b[39m   {\n\u001b[32m    273\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    274\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    275\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m56ec696a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    277\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    278\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    279\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom pathlib import Path\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    280\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    281\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport tarfile\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    282\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport urllib.request\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    283\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    284\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdef load_housing_data(): #defines a function that loads the housing data available as .tgz file on a github URL\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    285\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tarball_path = Path(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mdatasets/housing.tgz\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m) # where you will save your compressed data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    286\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    if not tarball_path.is_file():\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    287\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        Path(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mdatasets\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m).mkdir(parents=True, exist_ok=True) #create datasets folder if it does not exist\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    288\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        url = \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mhttps://github.com/ageron/data/raw/main/housing.tgz\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m # url of where you are getting your data from\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    289\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        urllib.request.urlretrieve(url, tarball_path) # gets the url content and saves it at location specified by tarball_path\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    290\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m        with tarfile.open(tarball_path) as housing_tarball: # opens saved compressed file as housing_tarball\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    291\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m            housing_tarball.extractall(path=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mdatasets\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m) # extracts the compressed content to datasets folder\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    292\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    return pd.read_csv(Path(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mdatasets/housing/housing.csv\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)) #uses panadas to read the csv file from the extracted content\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    293\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    294\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing = load_housing_data() #runsthe function defined above\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    295\u001b[39m    ]\n\u001b[32m    296\u001b[39m   },\n\u001b[32m    297\u001b[39m   {\n\u001b[32m    298\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33me678b056\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    300\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    301\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    302\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### If you\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve already downloaded and extracted the compressed file - then the following is all you need\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m    ]\n\u001b[32m    304\u001b[39m   },\n\u001b[32m    305\u001b[39m   {\n\u001b[32m    306\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    307\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    308\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m741051dd\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    309\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    310\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    311\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    312\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    313\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom pathlib import Path\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    314\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    315\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing = pd.read_csv(Path(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mdatasets/housing/housing.csv\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m))\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    316\u001b[39m    ]\n\u001b[32m    317\u001b[39m   },\n\u001b[32m    318\u001b[39m   {\n\u001b[32m    319\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    320\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfc852dd0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    321\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    322\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    323\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Take a Quick Look: housing data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    324\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    325\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWith Pandas, you can get a summary of the data by using the method `info()`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    326\u001b[39m    ]\n\u001b[32m    327\u001b[39m   },\n\u001b[32m    328\u001b[39m   {\n\u001b[32m    329\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    330\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    331\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33me780f081\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    332\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    333\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    334\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    335\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing.info()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m    ]\n\u001b[32m    337\u001b[39m   },\n\u001b[32m    338\u001b[39m   {\n\u001b[32m    339\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    340\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbb46ae03\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    341\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    342\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    343\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe result above tells you how many attributes (e.g. longitude, latitude) characterise the dataset. How many are there? The data type float64 is a numerical data type. So, the table above also tells you how many attributes are not numerical. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    344\u001b[39m    ]\n\u001b[32m    345\u001b[39m   },\n\u001b[32m    346\u001b[39m   {\n\u001b[32m    347\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    348\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    349\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m86c0ad40\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    350\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    351\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    352\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    353\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mocean_proximity\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m].value_counts() # tells you what values the column for `ocean_proximity` can take\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    354\u001b[39m    ]\n\u001b[32m    355\u001b[39m   },\n\u001b[32m    356\u001b[39m   {\n\u001b[32m    357\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    358\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    359\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfc4b5110\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    360\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    361\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    362\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    363\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing.hist(bins=50, figsize=(12, 8))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    364\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msave_fig(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mattribute_histogram_plots\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)  # extra code\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    365\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplt.show()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m    ]\n\u001b[32m    367\u001b[39m   },\n\u001b[32m    368\u001b[39m   {\n\u001b[32m    369\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    370\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m5900bf86\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    371\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    372\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    373\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinally you can run `housing.describe()` to get a summay of the data set `housing`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m    ]\n\u001b[32m    375\u001b[39m   },\n\u001b[32m    376\u001b[39m   {\n\u001b[32m    377\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    378\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    379\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mc3bc88c1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    380\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    381\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    382\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing.describe()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    384\u001b[39m    ]\n\u001b[32m    385\u001b[39m   },\n\u001b[32m    386\u001b[39m   {\n\u001b[32m    387\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    388\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m24805732\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    389\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    390\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    391\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAt this point you should stop looking at the data until you have set aside test data. This is to prevent inadvertent bias creeping into the machine learning process.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    392\u001b[39m    ]\n\u001b[32m    393\u001b[39m   },\n\u001b[32m    394\u001b[39m   {\n\u001b[32m    395\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mae81bc17\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    397\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    398\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    399\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 2-2: Download the Data: Example Image Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    401\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn contrast to tabular data, image data sets are not always read in using pandas. Technically you can do this (as the line below commented out suggests) but as there are no features human-friendly features (such as, median income etc.) - only pixel information, it does not always help to load it as a pandas dataframe, unless the model requires it to be so. Use the command `type` to see what data type `mnist` is - you can see that it is not a `pandas.core.frame.DataFrame`. Dataframes are not the preferred **data structure** for image data.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    402\u001b[39m    ]\n\u001b[32m    403\u001b[39m   },\n\u001b[32m    404\u001b[39m   {\n\u001b[32m    405\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    406\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    407\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mae8a9ac0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    408\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    409\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    410\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    411\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.datasets import fetch_openml\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    412\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    413\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    414\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmnist = fetch_openml(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmnist_784\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, as_frame=False, parser=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    415\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    416\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#mnist_dataframe = pd.DataFrame(data=mnist.data, columns=mnist.feature_names)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m    ]\n\u001b[32m    418\u001b[39m   },\n\u001b[32m    419\u001b[39m   {\n\u001b[32m    420\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    421\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2311961e\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    422\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    423\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    424\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Take a Quick Look: MNIST\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m    ]\n\u001b[32m    426\u001b[39m   },\n\u001b[32m    427\u001b[39m   {\n\u001b[32m    428\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    429\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfe3e0e2e\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    430\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    431\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    432\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe command `mnist.info()` will not work here, to get information about the dataset content, because it is not a pandas dataframe. However, for datasets in the `sklearn.datasets` universe, you can use the keyword `DECSR` - as demonstrated in the first code cell below. The `print` command can be used in conjunction to get some useful context of the dataset structue and origin. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    433\u001b[39m    ]\n\u001b[32m    434\u001b[39m   },\n\u001b[32m    435\u001b[39m   {\n\u001b[32m    436\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    437\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    438\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mf269f8bf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    439\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    440\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    441\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    442\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(mnist.DESCR)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    443\u001b[39m    ]\n\u001b[32m    444\u001b[39m   },\n\u001b[32m    445\u001b[39m   {\n\u001b[32m    446\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    447\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m3c410e32\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    448\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    449\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    450\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 2-3: Review the data description above with your group.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    451\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    452\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat is the size of each image?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    453\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    454\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExamine how LeCunn, Cortes, and Burges reorganised the NIST data as MNIST. Note that they remixed the data in two ways to create different a training dataset and test dataset. What they did do? Why do you think they did this? Was it justified? \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    455\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    456\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWrite down the results in a markdown cell below. I have already created a markdown cell below - just double click to edit the content.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m    ]\n\u001b[32m    458\u001b[39m   },\n\u001b[32m    459\u001b[39m   {\n\u001b[32m    460\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    461\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m55896634\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    462\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    463\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    464\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m***Mark Down cell for critique***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    465\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    466\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    467\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    468\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m3.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    469\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m4.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m    ]\n\u001b[32m    471\u001b[39m   },\n\u001b[32m    472\u001b[39m   {\n\u001b[32m    473\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    474\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mf2d5fba0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    475\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    476\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    477\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### To see a full list of keys other than `DESCR` which is available to this dataset You can use the command `mnist.keys()` to see more keys available - run the code below.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    478\u001b[39m    ]\n\u001b[32m    479\u001b[39m   },\n\u001b[32m    480\u001b[39m   {\n\u001b[32m    481\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    482\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    483\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m23a1d7e8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    484\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    485\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    486\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    487\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmnist.keys()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m    ]\n\u001b[32m    489\u001b[39m   },\n\u001b[32m    490\u001b[39m   {\n\u001b[32m    491\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    492\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33med70b9bc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    493\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    494\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    495\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 2-4: Identifying the Dimension of Images\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    496\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    497\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou may recognise some of the keys listed above for `mnist.keys()`. The keys `data` and `target` will return the image pixel data, and the labels (that is, the categories or classification) assigned to each of these images, respectively. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    498\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    499\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Create a code cell below to use these keys in Python, to use the `shape` command to verify the number of images in the dataset (70000) and how many features (e.g. pixels) represents the image (784). Print out the shape and the target categories. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    500\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    501\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI have created a cell for you below with the data and target assigned to the **variables** `images` and `categories`. Add a line to print out the shape of the images and the list a assigned categories. Single click on the area to start editing. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m    ]\n\u001b[32m    503\u001b[39m   },\n\u001b[32m    504\u001b[39m   {\n\u001b[32m    505\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    506\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    507\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1ae51ee3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    508\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    509\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    510\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    511\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# cell for python code \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    512\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    513\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimages = mnist.data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    514\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategories = mnist.target\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    515\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    516\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# insert lines below to print the shape of images and to print the categories.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    517\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    518\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(images.shape)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    519\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(categories)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    520\u001b[39m    ]\n\u001b[32m    521\u001b[39m   },\n\u001b[32m    522\u001b[39m   {\n\u001b[32m    523\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    524\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m7a8f61e6\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    525\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    526\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    527\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLet\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms take a look at one of the digits in the dataset - the first item.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    528\u001b[39m    ]\n\u001b[32m    529\u001b[39m   },\n\u001b[32m    530\u001b[39m   {\n\u001b[32m    531\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    532\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    533\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33meb52d34a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    534\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    535\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    536\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    537\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#extra code to visualise the image of digits\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    538\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    539\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport matplotlib.pyplot as plt\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    540\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    541\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## the code below defines a function plot_digit. The initial key work `def` stands for define, followed by function name.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    542\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## the function take one argument image_data in a parenthesis. This is followed by a colon. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    543\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Each line below that will be executed when the function is used. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    544\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## This cell only defines the function. The next cell uses the function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    545\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    546\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdef plot_digit(image_data): # defines a function so that you need not type all the lines below everytime you view an image\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    547\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    image = image_data.reshape(28, 28) #reshapes the data into a 28 x 28 image - before it was a string of 784 numbers\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    548\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    plt.imshow(image, cmap=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mbinary\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m) # show the image in black and white - binary.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    549\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    plt.axis(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33moff\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m) # ensures no x and y axes are displayed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    550\u001b[39m    ]\n\u001b[32m    551\u001b[39m   },\n\u001b[32m    552\u001b[39m   {\n\u001b[32m    553\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    554\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    555\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m6f02abd3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    556\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    557\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    558\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# visualise a selected digit with the following code\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    560\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    561\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msome_digit = mnist.data[0]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    562\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplot_digit(some_digit)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    563\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplt.show()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    564\u001b[39m    ]\n\u001b[32m    565\u001b[39m   },\n\u001b[32m    566\u001b[39m   {\n\u001b[32m    567\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    568\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m65e690ce\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    569\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    570\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    571\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Task 3: Setting Aside the Test Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    572\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    573\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTo set aside test data, you need to take shuffled and stratified samples. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    574\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    575\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Why Do We Shuffle\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    576\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    577\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe dataset you are working with could be ordered in a specific way (for example, all the data points in a specific class all at the top). If you select a percentage of 20\u001b[39m\u001b[38;5;132;01m% f\u001b[39;00m\u001b[33mrom the top, you could get data points in only specific classes. By shuffling we can avoid this. As it happens `sklearn` has a nifty function to allow you to split the data inclusive of splitting. This function is called `train_test_split`. See it in action below, using the housing data.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m    ]\n\u001b[32m    579\u001b[39m   },\n\u001b[32m    580\u001b[39m   {\n\u001b[32m    581\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    582\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    583\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m43c0ba19\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    584\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    585\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    586\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    587\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.model_selection import train_test_split\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    588\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    589\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtratio = 0.2 #to get 20\u001b[39m\u001b[38;5;132;01m% f\u001b[39;00m\u001b[33mor testing and 80\u001b[39m\u001b[38;5;132;01m% f\u001b[39;00m\u001b[33mor training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    590\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    591\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_set, test_set = train_test_split(housing, test_size=tratio, random_state=42) \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    592\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## assigning a number to random_state means that everytime you run this you get the same split, unless you change the data.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    593\u001b[39m    ]\n\u001b[32m    594\u001b[39m   },\n\u001b[32m    595\u001b[39m   {\n\u001b[32m    596\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    597\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m6040547c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    598\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    599\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    600\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Why Do We Stratify\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    601\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    602\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIf the dataset is skewed so that it contains more samples of a specific kind more than others, sampling randomly will result in your test data not representing the population you would like to test. An example of the estimated probability of getting a bad sample that does not reflect the actual population is provided below. The US population ratio of females in the census is 51.1\u001b[39m\u001b[33m%\u001b[39m\u001b[33m. The following is the probability of getting a sample with less than 48.5\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[33mr greater than 53.5\u001b[39m\u001b[38;5;132;01m% f\u001b[39;00m\u001b[33memales if you take a random sample withoput stratifying: approximately **10.71\u001b[39m\u001b[33m%\u001b[39m\u001b[33m** \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    603\u001b[39m    ]\n\u001b[32m    604\u001b[39m   },\n\u001b[32m    605\u001b[39m   {\n\u001b[32m    606\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    607\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    608\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m40e27820\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    609\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    610\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    611\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    612\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# extra code – shows another way to estimate the probability of bad sample\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    613\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    614\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport numpy as np\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    615\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    616\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msample_size = 1000\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    617\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mratio_female = 0.511\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    618\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    619\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnp.random.seed(42)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    620\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    621\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msamples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    622\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m((samples < 485) | (samples > 535)).mean()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    623\u001b[39m    ]\n\u001b[32m    624\u001b[39m   },\n\u001b[32m    625\u001b[39m   {\n\u001b[32m    626\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    627\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m38b4372c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    628\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    629\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    630\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 3.1: Stratified Sample: Housing Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    631\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    632\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe following code adds a column to the `housing` data to create bins of data according to interval brackets of median income of districts. This is a first step to creating a stratified sample across different income brackets.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    633\u001b[39m    ]\n\u001b[32m    634\u001b[39m   },\n\u001b[32m    635\u001b[39m   {\n\u001b[32m    636\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    637\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    638\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m75b0cd11\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    639\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    640\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    641\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    642\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport numpy as np\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    643\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    644\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    645\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mincome_cat\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m] = pd.cut(housing[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_income\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m],\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    646\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    647\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                               labels=[1, 2, 3, 4, 5])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m    ]\n\u001b[32m    649\u001b[39m   },\n\u001b[32m    650\u001b[39m   {\n\u001b[32m    651\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    652\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1ed7fbfe\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    653\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    654\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    655\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe following code uses the above bins to implement startified sampling - that is, it will randomly sample 20\u001b[39m\u001b[33m%\u001b[39m\u001b[33m (because we set test ratio `tratio` to 0.2) from each income bracket defined above.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m    ]\n\u001b[32m    657\u001b[39m   },\n\u001b[32m    658\u001b[39m   {\n\u001b[32m    659\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    660\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    661\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mc69cc591\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    662\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    663\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    664\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    665\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.model_selection import train_test_split\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    666\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    667\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtratio = 0.2 #to get 20\u001b[39m\u001b[38;5;132;01m% f\u001b[39;00m\u001b[33mor testing and 80\u001b[39m\u001b[38;5;132;01m% f\u001b[39;00m\u001b[33mor training\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    668\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    669\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstrat_train_set, strat_test_set = train_test_split(housing, test_size=tratio, stratify=housing[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mincome_cat\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m], random_state=42)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    670\u001b[39m    ]\n\u001b[32m    671\u001b[39m   },\n\u001b[32m    672\u001b[39m   {\n\u001b[32m    673\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    674\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m19d7fcdc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    675\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    676\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    677\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe code below prints out the proportion of each income category in the stratified test set above.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    678\u001b[39m    ]\n\u001b[32m    679\u001b[39m   },\n\u001b[32m    680\u001b[39m   {\n\u001b[32m    681\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    682\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    683\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m57103983\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    684\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    685\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    686\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    687\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstrat_test_set[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mincome_cat\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m].value_counts() / len(strat_test_set) #Prints out in order of the highest proportion first.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    688\u001b[39m    ]\n\u001b[32m    689\u001b[39m   },\n\u001b[32m    690\u001b[39m   {\n\u001b[32m    691\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    692\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mb0b1d994\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    693\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    694\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    695\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNote the attribute `random_state`. Setting this to a specific number like 42 **keeps the split the same everytime you run the code**. Keep in mind that it will not stay the same if you change the underlying dataset (e.g. adding more). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    696\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    697\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDiscuss with your peer group, why a stratified sample based on median income is reasonable. Create a markdown cell below to report on the results of the discussion. I have already create one below, so just double click to edit.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    698\u001b[39m    ]\n\u001b[32m    699\u001b[39m   },\n\u001b[32m    700\u001b[39m   {\n\u001b[32m    701\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    702\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m6718cca3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    703\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    704\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    705\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m***Markdown cell***\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    706\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    707\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    708\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    709\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m3.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    710\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m4.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    711\u001b[39m    ]\n\u001b[32m    712\u001b[39m   },\n\u001b[32m    713\u001b[39m   {\n\u001b[32m    714\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    715\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    716\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33md55a6a4f\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    717\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    718\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    719\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    720\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype(mnist.data)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    721\u001b[39m    ]\n\u001b[32m    722\u001b[39m   },\n\u001b[32m    723\u001b[39m   {\n\u001b[32m    724\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    725\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m270dbdb6\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    726\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    727\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    728\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 3.2: Setting Aside Test Set for Image Data \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    729\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    730\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn the case of `mnist` the data is already cleaned prepared, scaled and ordered, so that the training data is the first 60,000 images, followed by the test data which is the last 10,000 images. So you need not shuffle and stratify nor use `train_test_split`. Instead, you can use the following code to set aside your test dataset. The data type of `mnist.data` is `numpy.ndarray` (you can verify this with the command `type`). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    731\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- By using a colon and then 60000 in a square bracket after `mnist.data`, you are telling the computer that you want all the items up until the 60000th item (not including the 60000th) in the array `mnist.data`. We assign this to the **variable** `X_train`. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    732\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Likewise, the first 60000 categories corresponding the the first 60000 images are assigned to the **variable** `y_train`. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    733\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- By using a colon after 60000, you are telling the computer you would like all the items from the 60000th onwards.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    734\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    735\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIt is machine learning convention to use upper case `X` for variable names associated with data and lower case `y` in the variable names associated to labels (or categories/classes).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    736\u001b[39m    ]\n\u001b[32m    737\u001b[39m   },\n\u001b[32m    738\u001b[39m   {\n\u001b[32m    739\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    740\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    741\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m3ab94357\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    742\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    743\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    744\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    745\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_train = mnist.data[:60000]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    746\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33my_train = mnist.target[:60000]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    747\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    748\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_test = mnist.data[60000:]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    749\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33my_test = mnist.target[60000:]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    750\u001b[39m    ]\n\u001b[32m    751\u001b[39m   },\n\u001b[32m    752\u001b[39m   {\n\u001b[32m    753\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    754\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m4fc35af8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    755\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    756\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    757\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Task 4: Selecting and Training a Model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    758\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    759\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou are finally ready to select and train your model. In the following code, we will use linear **regression for the prediction of district housing prices**, and a **convolutional neural network** for classification of hand written digits. For linear regression, we will use `Scikit-Learn`. For the convolutional neural networks we will use the `tensorflow` library with `keras`. Regardless of the model, the general flow is similar:  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    760\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    761\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Import the model from the relevant library. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    762\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Create an untrained model instance. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    763\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Fit the model to your training data.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    764\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    765\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 4-1: Housing Data and Linear Regression\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    766\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    767\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWith linear regression, you need data, whose values are continuous - not discrete values such as categories. Note that it is not enough for the values to be numbers, which can also be categories (for example, your place in a queue is a number but is never a fraction like 1.33). The feature `income_cat` is another category expressed as a number. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    768\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    769\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBefore doing anything else, let\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms assign a copy of the stratified training set we created earlier to the variable `housing`. You should always work with copies of data and never look at the test set in case we inadvertently use information in the test to improve the performance (**data snooping bias**).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    770\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    771\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTo do this use the following code.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    772\u001b[39m    ]\n\u001b[32m    773\u001b[39m   },\n\u001b[32m    774\u001b[39m   {\n\u001b[32m    775\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    776\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    777\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m9b6cbdb3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    778\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    779\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    780\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    781\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing = strat_train_set.copy()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    782\u001b[39m    ]\n\u001b[32m    783\u001b[39m   },\n\u001b[32m    784\u001b[39m   {\n\u001b[32m    785\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    786\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m90f59211\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    787\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    788\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    789\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 1: Checking Correlations: Training Set\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    790\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    791\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLinear regression in essence works by picking up on correlations between features. So it can be useful to explore the correlations especially between the target value you are trying to predict `median_house_value` and the other features in the dataset, e.g. `median_income`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    792\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    793\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe training set we have is of type `pandas.DataFrame`.  For pandas, dataframes have the function `corr` which calculates the correlations for you. The code is below - first it calculates all the correlations between all the pairs of features and saves it in variable `corr_matrix`. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    794\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    795\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWe can take a look at correlations for `median_house_value` by using that feature name in a square bracket (**with quotation marks!**). the last part `sort_values(ascending=False)` sorts the correlation to display it in descending order of correlation (that is, most correlated fetures are listed first).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    796\u001b[39m    ]\n\u001b[32m    797\u001b[39m   },\n\u001b[32m    798\u001b[39m   {\n\u001b[32m    799\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    800\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    801\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m4767a968\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    802\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    803\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    804\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    805\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcorr_matrix = housing.corr(numeric_only=True) # argument is so that it only calculates for numeric value features\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    806\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcorr_matrix[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_house_value\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m].sort_values(ascending=False)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    807\u001b[39m    ]\n\u001b[32m    808\u001b[39m   },\n\u001b[32m    809\u001b[39m   {\n\u001b[32m    810\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    811\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33me741e0d8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    812\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    813\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    814\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 2: Visualise the Correlations\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    815\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    816\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPandas also can visualise these correlations as a graph for you. In the code below, we have selected four features (see the variable with that name), to 4 x 4 grid of graphs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    817\u001b[39m    ]\n\u001b[32m    818\u001b[39m   },\n\u001b[32m    819\u001b[39m   {\n\u001b[32m    820\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    821\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    822\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1b7facae\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    823\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    824\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    825\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    826\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom pandas.plotting import scatter_matrix\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    827\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    828\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeatures = [\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_house_value\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_income\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtotal_rooms\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    829\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m              \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mhousing_median_age\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    830\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscatter_matrix(housing[features], figsize=(12, 8))\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    831\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#save_fig(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mscatter_matrix_plot\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    832\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    833\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#The line above is extra code you can uncomment (remove the hash at the begining) to save the image.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    834\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#But, to use this, make sure you ran the code at the beginning of this notebook defining the save_fig function\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    835\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    836\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mplt.show()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    837\u001b[39m    ]\n\u001b[32m    838\u001b[39m   },\n\u001b[32m    839\u001b[39m   {\n\u001b[32m    840\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    841\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m94c4e919\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    842\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    843\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    844\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 3: Separate the Target Labels from Your Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    845\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    846\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn any machine learning task, you need to provide the data and the target Label separately to the machine learning algorithm. Otherwise, they have no way of knowing which of the features is the target label. In our scenario, the label for the housing data is the `median_house_value`. When your data is in a padas dataframe format, you can simply 1) drop the column with the label to get the data, and, 2) get the column for the target label, to get the labels.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    847\u001b[39m    ]\n\u001b[32m    848\u001b[39m   },\n\u001b[32m    849\u001b[39m   {\n\u001b[32m    850\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    851\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    852\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m610bec83\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    853\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    854\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    855\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    856\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing = strat_train_set.drop(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_house_value\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, axis=1) ## 1)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    857\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_labels = strat_train_set[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_house_value\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m].copy() ## 2)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    858\u001b[39m    ]\n\u001b[32m    859\u001b[39m   },\n\u001b[32m    860\u001b[39m   {\n\u001b[32m    861\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    862\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m57a9e4b6\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    863\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    864\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    865\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 4: Look for Missing Values in the Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    866\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    867\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhen working with tabular data, it is quite common to find that some rows are missing values for some of the columns. If you run the `info` command for dataframes (we\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve done this in [Task 2-1](#Task-2-1:-Download-the-Data:-Example-Tabular-Data) above!). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    868\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    869\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Running the code will show the total number of entries. By comparing that number to the number of Non-Null entries for each feature (e.g. `total_bedrooms`) you can see whether there are missing values. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    870\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- If there are no missing values, these numbers should be the same!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    871\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    872\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHow many values are missing for the number of `total_bedrooms\u001b[39m\u001b[33m'\u001b[39m\u001b[33m? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    873\u001b[39m    ]\n\u001b[32m    874\u001b[39m   },\n\u001b[32m    875\u001b[39m   {\n\u001b[32m    876\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    877\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    878\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m3bffc540\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    880\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    881\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    882\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing.info()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m    ]\n\u001b[32m    884\u001b[39m   },\n\u001b[32m    885\u001b[39m   {\n\u001b[32m    886\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    887\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m31181096\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    888\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    889\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    890\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 5: Handling Missing Values\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    891\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    892\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTo handle the missing values, you need a code in place to tell the machine what to do if there are missing values. In-depth discussion of handling missing values is beyond the scope of this course, but there are three common ways of handing these:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    893\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    894\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- (Option 1) Drop the row with missing value. This causes you to lose data points. In our scenario with the housing data, 168 rows will be removed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    895\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- (Option 2) Drop the column with missing values. This causes you to lose one of your features.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    896\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- (Option 3) Fill in the missing value with some value such as the median or mean or fixed value that makes sense. This is called **imputing**.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    897\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    898\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDepending on which approach you take, the performance of your AI could be different. Also, note that, **with Option 1, you will have to remove the corresponding rows in `housing_labels` before using these in a machine learning task**. Following are codes from each of these approaches. Read the comments included in the code for understanding what each cell is doing.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    899\u001b[39m    ]\n\u001b[32m    900\u001b[39m   },\n\u001b[32m    901\u001b[39m   {\n\u001b[32m    902\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    903\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    904\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mb0ee6f1d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    905\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    906\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    907\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    908\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# this is the code for Option 1 above. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    909\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option1 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt mess up the original data.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    910\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    911\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option1.dropna(subset=[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtotal_bedrooms\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m], inplace=True)  # option 1 - dropping the rows where total_bedroom is missing values.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    912\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    913\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option1.info() #look for missing values after rows have been dropped\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    914\u001b[39m    ]\n\u001b[32m    915\u001b[39m   },\n\u001b[32m    916\u001b[39m   {\n\u001b[32m    917\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    918\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    919\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33me4e11d2f\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    920\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    921\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    922\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    923\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option2 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt mess up the original data.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    924\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    925\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option2.drop(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtotal_bedrooms\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, axis=1, inplace=True)  # option 2 - dropping the column associated with total_bedrooms\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    926\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    927\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option2.info() # checking for missing values in the new data after column has been dropped\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    928\u001b[39m    ]\n\u001b[32m    929\u001b[39m   },\n\u001b[32m    930\u001b[39m   {\n\u001b[32m    931\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    932\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    933\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m74dba58c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    934\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    935\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    936\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    937\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option3 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt mess up the original data.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    938\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    939\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmedian = housing[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtotal_bedrooms\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m].median() # calculating mean of the value for total_bedrooms to use in filling missing values\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    940\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option3[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mtotal_bedrooms\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m].fillna(median, inplace=True)  # option 3 - filling missing values with the median\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    941\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    942\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_option3.info()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    943\u001b[39m    ]\n\u001b[32m    944\u001b[39m   },\n\u001b[32m    945\u001b[39m   {\n\u001b[32m    946\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    947\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m0cf9695a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    948\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    949\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    950\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#### You can also use `SimpleImputer` from the `sklearn.impute` library to fill missing values with the median\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    951\u001b[39m    ]\n\u001b[32m    952\u001b[39m   },\n\u001b[32m    953\u001b[39m   {\n\u001b[32m    954\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    955\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    956\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2b28f434\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    957\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    958\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    959\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    960\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.impute import SimpleImputer\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    962\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimputer = SimpleImputer(strategy=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m) # initialises the imputer\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    963\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    964\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_num = housing.select_dtypes(include=[np.number]) ## includes only numeric features in the data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    965\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    966\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimputer.fit(housing_num) #calculates the median for each numeric feature so that the imputer can use them\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    967\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    968\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_num[:] = imputer.transform(housing_num) # the imputer uses the median to fill the missing values and saves the result in variable X\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    969\u001b[39m    ]\n\u001b[32m    970\u001b[39m   },\n\u001b[32m    971\u001b[39m   {\n\u001b[32m    972\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    973\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m32c1a0b1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    974\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    975\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    976\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 6: Scaling Your Features\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    977\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    978\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMachine learning algorithms learn better when similar scales are used across all the features. For example, the numeric range of values for `total_rooms` will be totally different from `median_income`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    979\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    980\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTest this with he **min** and **max** values after running the pandas `describe()` function. Code below.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    981\u001b[39m    ]\n\u001b[32m    982\u001b[39m   },\n\u001b[32m    983\u001b[39m   {\n\u001b[32m    984\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    985\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m    986\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mead8cda8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    987\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    988\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m    989\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    990\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_num.describe()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m    ]\n\u001b[32m    992\u001b[39m   },\n\u001b[32m    993\u001b[39m   {\n\u001b[32m    994\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    995\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfb924d5b\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    996\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m    997\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    998\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou can see that all the features have very different ranges. Bringing these in alignment is called **feature scaling**. There are a number of ways to scale features. Scikit-Learn provides something called MinMaxScaler which scales the values to fit into a range defined by you. Below, the code is provided for when you are fitting it into the range from -1 to 1. AI algorithms often like the mean to be placed at zero, so best to set a range with zero as the mid point value. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    999\u001b[39m    ]\n\u001b[32m   1000\u001b[39m   },\n\u001b[32m   1001\u001b[39m   {\n\u001b[32m   1002\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1003\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1004\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m396bcce8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1005\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1006\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1007\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1008\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.preprocessing import MinMaxScaler # get the MinMaxScaler\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1009\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1010\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmin_max_scaler = MinMaxScaler(feature_range=(-1, 1)) # setup an instance of a scaler\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1011\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)# use the scaler to transform the data housing_num\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1012\u001b[39m    ]\n\u001b[32m   1013\u001b[39m   },\n\u001b[32m   1014\u001b[39m   {\n\u001b[32m   1015\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1016\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mafc23108\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1017\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1018\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1019\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAlternatively, Scikit-Learn also provides a method called StandardScaler. This method tries normalise the distributional characteristics by considering mean and standard deviation for each feature, and normalising the values to have standard deviation 1. But, even without knowing the mathematical details, we can simply employ the tools provided by `sklearn` - example below.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m    ]\n\u001b[32m   1021\u001b[39m   },\n\u001b[32m   1022\u001b[39m   {\n\u001b[32m   1023\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1024\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1025\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33md3eb2e06\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1026\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1027\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1028\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1029\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.preprocessing import StandardScaler\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1030\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1031\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstd_scaler = StandardScaler()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1032\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_num_std_scaled = std_scaler.fit_transform(housing_num)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1033\u001b[39m    ]\n\u001b[32m   1034\u001b[39m   },\n\u001b[32m   1035\u001b[39m   {\n\u001b[32m   1036\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1037\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1038\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m43bd704e\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1039\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1040\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1041\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1042\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhousing_num[:]=std_scaler.fit_transform(housing_num)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1043\u001b[39m    ]\n\u001b[32m   1044\u001b[39m   },\n\u001b[32m   1045\u001b[39m   {\n\u001b[32m   1046\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1047\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1018f320\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1048\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1049\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1050\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 7: Train a Linear Regression Model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1051\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1052\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn the first instance we will use the data resulting from the `SimpleImputer` (with the **median** as a strategy) and use   `StandardScaler` for scaling the features. Before we train the Linear Regression model for predicting median housing prices for districts, we need to also apply the scaling to the target labels (in our case, the known median housing prices). The code is provided below.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1053\u001b[39m    ]\n\u001b[32m   1054\u001b[39m   },\n\u001b[32m   1055\u001b[39m   {\n\u001b[32m   1056\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1057\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1058\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m483b52a3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1059\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1060\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1061\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1062\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.preprocessing import StandardScaler #This line is not necessary if you ran this prior to running this cell. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1063\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#We are however including it here for completeness sake.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1064\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1065\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtarget_scaler = StandardScaler() #instance of Scaler\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1066\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscaled_labels = target_scaler.fit_transform(housing_labels.to_frame()) #calculate the mean and standard deviation and use it to transform the target labels.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1067\u001b[39m    ]\n\u001b[32m   1068\u001b[39m   },\n\u001b[32m   1069\u001b[39m   {\n\u001b[32m   1070\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1071\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m2ed4b20f\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1072\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1073\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1074\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Training Step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1075\u001b[39m    ]\n\u001b[32m   1076\u001b[39m   },\n\u001b[32m   1077\u001b[39m   {\n\u001b[32m   1078\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1079\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1080\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfdc4e78d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1081\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1082\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1083\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1084\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.linear_model import LinearRegression #get the library from sklearn.linear model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1085\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1086\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel = LinearRegression() #get an instance of the untrained model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1087\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel.fit(housing_num, scaled_labels)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1088\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#model.fit(housing[[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_income\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m]], scaled_labels) #fit it to your data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1089\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#some_new_data = housing[[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_income\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m]].iloc[:5]  # pretend this is new data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1090\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1091\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#scaled_predictions = model.predict(some_new_data)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1092\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#predictions = target_scaler.inverse_transform(scaled_predictions)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1093\u001b[39m    ]\n\u001b[32m   1094\u001b[39m   },\n\u001b[32m   1095\u001b[39m   {\n\u001b[32m   1096\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1097\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1098\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m129a1e49\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1099\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1100\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1101\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1102\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msome_new_data = housing_num.iloc[:5] #pretend this is new data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#some_new_data = housing[[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mmedian_income\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m]].iloc[:5]  # pretend this is new data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1105\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscaled_predictions = model.predict(some_new_data)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1106\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpredictions = target_scaler.inverse_transform(scaled_predictions)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1107\u001b[39m    ]\n\u001b[32m   1108\u001b[39m   },\n\u001b[32m   1109\u001b[39m   {\n\u001b[32m   1110\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1111\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1112\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mebd4851b\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1113\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1114\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1115\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(predictions, housing_labels.iloc[:5])\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1117\u001b[39m    ]\n\u001b[32m   1118\u001b[39m   },\n\u001b[32m   1119\u001b[39m   {\n\u001b[32m   1120\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1121\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1122\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m3980d05c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1123\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1124\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1125\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1126\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# extra code – computes the error ratios discussed in the book\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1127\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33merror_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1128\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m.join([f\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m{\u001b[39m\u001b[33m100 * ratio:.1f}\u001b[39m\u001b[33m%\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m for ratio in error_ratios]))\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1129\u001b[39m    ]\n\u001b[32m   1130\u001b[39m   },\n\u001b[32m   1131\u001b[39m   {\n\u001b[32m   1132\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1133\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mf01ebd5a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1134\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1135\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1136\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 8: Cross Validation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1137\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1138\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAs mentioned in Lecture 4 - pre-recorded lecture - having one training set and one test set to check performance is limited in producing a robust AI model. What you really want to see is a stable performance across many training sets and test sets. IN the first stance you want to test the model on the training set.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1139\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1140\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mOne way to evaluate your model before testing on the new data (the data you set aside as your test data) is cross validation. This where you split your training data into many pieces, then leave on of the pieces out for testing.The code below does that!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1141\u001b[39m    ]\n\u001b[32m   1142\u001b[39m   },\n\u001b[32m   1143\u001b[39m   {\n\u001b[32m   1144\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1145\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1146\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcf15d382\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1147\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1148\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1149\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1150\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.model_selection import cross_val_score\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1151\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1152\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrmses = -cross_val_score(model, housing_num, scaled_labels,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1153\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m                              scoring=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mneg_root_mean_squared_error\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, cv=10)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1154\u001b[39m    ]\n\u001b[32m   1155\u001b[39m   },\n\u001b[32m   1156\u001b[39m   {\n\u001b[32m   1157\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1158\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1159\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m30ea58a8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1160\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1161\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1162\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1163\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpd.Series(rmses).describe()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1164\u001b[39m    ]\n\u001b[32m   1165\u001b[39m   },\n\u001b[32m   1166\u001b[39m   {\n\u001b[32m   1167\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1168\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mc936dc9a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1169\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1170\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1171\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 4-2: Hand Written Digit Classification\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1172\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1173\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAs mentioned earlier, as an example, for the hand written digits, we will use a specific kind of neural network model called a **Convolutional Neural Network (CNN)** model. In the lectures, we learned about general neural networks but not CNN. If you want to get a feel for CNNs, you can watch the Stat Quest Video [Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)](https://www.youtube.com/watch?v=HGwBXDKFk9I). If you feel confident to go deeper, Chapters 19 and 22 of Russell and Norvig\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms Book [\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mArtificial Intelligence: a modern approach\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m](https://eleanor.lib.gla.ac.uk/record=b3897063) is an excellent read, not to mention Géron\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms book [\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mHands-On Machine Learning with Scikit-Learn, Keras & Tensorflow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m](https://eleanor.lib.gla.ac.uk/record=b4094676).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1174\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1175\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFor this task, **we will move away from `sklearn` and use `tensorflow` and `keras` instead**. Tensorflow and Keras are popular libraries recognised for their usefulness in building neural networks quickly. Although we already loaded the data from `sklearn`, in the code below, we will get it again from `tensorflow.keras.datasets`. This will allow you to experience getting data from another library and also make it easier to work with the subsequent code because everything will happen with tensorflow. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1176\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1177\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe data is already fairly organised, so, the data cleaning part of the operation can be abbreviated. This is however not a characteristic of image data. It is a characteristic of **curated data** which is not the same as real world messy data (such as the housing data from earlier). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1178\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1179\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe code for importing the libraries and getting the data has been included below. To get these to work, **you will need to have your environment installed with `tensorflow` and `keras`**. In the first line of the first code cell below, you will notice that `tensorflow` is imported as `tf`. This is a recognised convention in the machine learning community. Adopting this convention makes your code more readable for this community. Once you have imported the library that way, `tf` will be used subsequently instead of `tensorflow`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1180\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1181\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 1: Get the Data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1182\u001b[39m    ]\n\u001b[32m   1183\u001b[39m   },\n\u001b[32m   1184\u001b[39m   {\n\u001b[32m   1185\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1186\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1187\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m542b6908\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1188\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1189\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1190\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1191\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport tensorflow as tf\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1192\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1193\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmnist = tf.keras.datasets.mnist.load_data()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1194\u001b[39m    ]\n\u001b[32m   1195\u001b[39m   },\n\u001b[32m   1196\u001b[39m   {\n\u001b[32m   1197\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1198\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m8b89621d\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1199\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1200\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1201\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 2: Review What the Data Looks Like  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1202\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1203\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou can review information about what this dataset looks like at the Keras page for the [MNIST digits classification dataset](https://keras.io/api/datasets/mnist/). The page makes it clear that `mnist` above is organised as a data type called **tuple** - something that looks like `(a,b)`. The `a` and `b` are tuples themselves, representing training and test data, respectively. Check first that mnist is a **tuple** with following line of code.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1204\u001b[39m    ]\n\u001b[32m   1205\u001b[39m   },\n\u001b[32m   1206\u001b[39m   {\n\u001b[32m   1207\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1208\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1209\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m45cdcbee\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1210\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1211\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1212\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1213\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(type(mnist))\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1214\u001b[39m    ]\n\u001b[32m   1215\u001b[39m   },\n\u001b[32m   1216\u001b[39m   {\n\u001b[32m   1217\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1218\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1adf9270\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1219\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1220\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1221\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe **Keras webpages are useful** for looking up and getting information about wide range of keras commands you might encouter in machine learning programs. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1222\u001b[39m    ]\n\u001b[32m   1223\u001b[39m   },\n\u001b[32m   1224\u001b[39m   {\n\u001b[32m   1225\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1226\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1805f5f4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1227\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1228\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1229\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 3: How to Get the Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1230\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1231\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTo get the data out of `a` and `b`, run the following code. Read the comment for explanation. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1232\u001b[39m    ]\n\u001b[32m   1233\u001b[39m   },\n\u001b[32m   1234\u001b[39m   {\n\u001b[32m   1235\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1236\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1237\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mc621149f\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1238\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1239\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1240\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1241\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m(X_train_full, y_train_full), (X_test, y_test) = mnist \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1242\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# (X_train_full, y_train_full) is the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtuple\u001b[39m\u001b[33m'\u001b[39m\u001b[33m related to `a` and (X_test, y_test) is the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtuple\u001b[39m\u001b[33m'\u001b[39m\u001b[33m related to `b`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1243\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# X_train_full is the full training data and y_train_full are the corresponding labels \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1244\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# - labels indicate what digit the image is of, for example 5 if it is an image of a handwritten 5.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1245\u001b[39m    ]\n\u001b[32m   1246\u001b[39m   },\n\u001b[32m   1247\u001b[39m   {\n\u001b[32m   1248\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1249\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m9a62b91a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1250\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1251\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1252\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 4: Scaling the Pixel Values (the features)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1253\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1254\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn dealing with images, there are four main comsiderations that most frequently arise: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1255\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- 1) input size of the image (height and width in terms of pixels)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1256\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- 2) whether you want to move the pixels so that the image is centered in the middle\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1257\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- 3) scaling the value of the pixels to be in a specified range. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1258\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1259\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe neural network we will use will works best with pixel values between 0 and 1. Pixels in a black and white image usually have values between 0 and 255. The code below simply rescales these, dividing by 255. There are other ways of scaling this, similar to when we scaled the feature values of the `housing` data.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1260\u001b[39m    ]\n\u001b[32m   1261\u001b[39m   },\n\u001b[32m   1262\u001b[39m   {\n\u001b[32m   1263\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1264\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1265\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m7c876f2c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1266\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1267\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1268\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1269\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_train_full = X_train_full / 255.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1270\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_test = X_test / 255.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1271\u001b[39m    ]\n\u001b[32m   1272\u001b[39m   },\n\u001b[32m   1273\u001b[39m   {\n\u001b[32m   1274\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1275\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mbf244fc8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1276\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1277\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1278\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 5: Split the Training Data into Training and Validation Data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1279\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1280\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWe already have data split into training and test data. The **validation data is split from the training data** and used to evaluate the performance during training. This is **different from test data** which is completely new data not seen during training or fine tuning. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1281\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1282\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTest data is used for the final test before publishing the results. In fact in many competitions, the test data is **withheld behind an application interface** so that contestants cannot engage in **data snooping**. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1283\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1284\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe code below takes the last 5000 images for validation data. The second line does the same for the corresponding labels.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1285\u001b[39m    ]\n\u001b[32m   1286\u001b[39m   },\n\u001b[32m   1287\u001b[39m   {\n\u001b[32m   1288\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1289\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1290\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m9197eb2f\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1291\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1292\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1293\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1294\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1295\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33my_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1296\u001b[39m    ]\n\u001b[32m   1297\u001b[39m   },\n\u001b[32m   1298\u001b[39m   {\n\u001b[32m   1299\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1300\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33md405ef72\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1301\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1302\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1303\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 5: Increasing Dimension to Include Colour Channels\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1304\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1305\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAn image is usually represented as a (width x height) block of pixels. When presenting your images to the neural network, you need to add an extra dimension to your image representation, to indicate the number of colour channels your images are using. Normally, for a greyscale image this would be 1, while for a RBG colour image it would be 3. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1306\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1307\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAll in all you will be submitting something that has shape like `(N, W, H, C)` where `N` is number of images, `W` is the width of any one image, `H` is the height of any one image, and `C` is the number channels (1 for greyscale, 3 for colour). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1308\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1309\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAll your images are expected to be the same size as it enters the neural network. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1310\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1311\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe mnist dataset currently has a shape like `(N, W, H)`. Your numpy library allows you to add the required extra dimension. The code below does this.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1312\u001b[39m    ]\n\u001b[32m   1313\u001b[39m   },\n\u001b[32m   1314\u001b[39m   {\n\u001b[32m   1315\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1316\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1317\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33meb5d4722\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1318\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1319\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1320\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1321\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport numpy as np # you won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt need to run this line if you ran it before in this notebook. But for completeness.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1322\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1323\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_train = X_train[..., np.newaxis] #adds a dimension to the image training set - the three dots means keeping everything else the same.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1324\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_valid = X_valid[..., np.newaxis]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1325\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mX_test = X_test[..., np.newaxis]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1326\u001b[39m    ]\n\u001b[32m   1327\u001b[39m   },\n\u001b[32m   1328\u001b[39m   {\n\u001b[32m   1329\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1330\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m6f9f3aba\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1331\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1332\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1333\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 6: Build the Neural Network and Fit it to the Data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1334\u001b[39m    ]\n\u001b[32m   1335\u001b[39m   },\n\u001b[32m   1336\u001b[39m   {\n\u001b[32m   1337\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1338\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1339\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33ma2a69e1c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1340\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1341\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1342\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1343\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtf.keras.backend.clear_session()\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1344\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1345\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtf.random.set_seed(42)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1346\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnp.random.seed(42)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1347\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1348\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Unlike scikit-learn, with tensorflow and keras, the model is built by defining each layer of the neural network.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1349\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Below, everytime tf.keras.layers is called it is building in another layer\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1350\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1351\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel = tf.keras.Sequential([\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1352\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.Conv2D(32, kernel_size=3, padding=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33msame\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, activation=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mrelu\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, kernel_initializer=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mhe_normal\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1353\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.Conv2D(64, kernel_size=3, padding=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33msame\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, activation=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mrelu\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, kernel_initializer=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mhe_normal\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1354\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.MaxPool2D(),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1355\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.Flatten(),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1356\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.Dropout(0.25),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1357\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.Dense(128, activation=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mrelu\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, kernel_initializer=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mhe_normal\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1358\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.Dropout(0.5),\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1359\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    tf.keras.layers.Dense(10, activation=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33msoftmax\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1360\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m])\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1361\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel.compile(loss=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, optimizer=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mnadam\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1362\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m              metrics=[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33maccuracy\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m])\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1363\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1364\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1365\u001b[39m    ]\n\u001b[32m   1366\u001b[39m   },\n\u001b[32m   1367\u001b[39m   {\n\u001b[32m   1368\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1369\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1370\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33md356c6f5\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1371\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1372\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1373\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1374\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel.summary() # not necessary for the machine learning task.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1375\u001b[39m    ]\n\u001b[32m   1376\u001b[39m   },\n\u001b[32m   1377\u001b[39m   {\n\u001b[32m   1378\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1379\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m022105d9\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1380\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1381\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe summary above is not easy to read initially but it is a presentation of each layer. The numbers at the bottom tell you how many parameters need learning in this model. The visualisation can be useful later when you get more used to neural networks if you should continue on to Semester 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1383\u001b[39m    ]\n\u001b[32m   1384\u001b[39m   },\n\u001b[32m   1385\u001b[39m   {\n\u001b[32m   1386\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1387\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m0e414d05\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1388\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1389\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1390\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Step 7: Train and Evaluate the Model \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1391\u001b[39m    ]\n\u001b[32m   1392\u001b[39m   },\n\u001b[32m   1393\u001b[39m   {\n\u001b[32m   1394\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1395\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1396\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mf36a2f27\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1397\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1398\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1399\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1400\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1401\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel.evaluate(X_test, y_test)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1402\u001b[39m    ]\n\u001b[32m   1403\u001b[39m   },\n\u001b[32m   1404\u001b[39m   {\n\u001b[32m   1405\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1406\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdfc4b587\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1407\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1408\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1409\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Comparing with Another Model\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1410\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1411\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBelow you are provided with code for using something called **Stochastic Gradient Decent Classifier**. This model applies the stochastic gradient descent optimiser (cf. the **nadam** optimiser used with the CNN above) with any number of algorithms but by default it applies it to a **Support Vector Machine**. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1412\u001b[39m    ]\n\u001b[32m   1413\u001b[39m   },\n\u001b[32m   1414\u001b[39m   {\n\u001b[32m   1415\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1416\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1417\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m568ed072\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1418\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1419\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1420\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1421\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# getting the data again from Scikit-Learn, so that we know the image dimens fit for the model!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1422\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1423\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.datasets import fetch_openml\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1424\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimport pandas as pd\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1425\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1426\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmnist = fetch_openml(\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmnist_784\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, as_frame=False, parser=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1427\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1428\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# getting the data and the categories for the data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1429\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimages = mnist.data\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1430\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategories = mnist.target\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1431\u001b[39m    ]\n\u001b[32m   1432\u001b[39m   },\n\u001b[32m   1433\u001b[39m   {\n\u001b[32m   1434\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1435\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1185aac1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1436\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1437\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1438\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m**Normally, we would set aside the test data**. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1439\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1440\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBut in this experiement we will abbreviate and use the entire data and evaluate using cross validation, especially since we are not intending, on this occasion, to develop our model with the validation step. **Note that running this might take a while - so be patient!**\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1441\u001b[39m    ]\n\u001b[32m   1442\u001b[39m   },\n\u001b[32m   1443\u001b[39m   {\n\u001b[32m   1444\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1445\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1446\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m5a621d01\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1447\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1448\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1449\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1450\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.linear_model import SGDClassifier\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1451\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom sklearn.model_selection import cross_val_score\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1452\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1453\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msgd_clf = SGDClassifier(random_state=42)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1454\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1455\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#cross validation on training data for fit accuracy\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1456\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1457\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccuracy = cross_val_score(sgd_clf, images, categories, cv=10)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1458\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1459\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(accuracy)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1460\u001b[39m    ]\n\u001b[32m   1461\u001b[39m   },\n\u001b[32m   1462\u001b[39m   {\n\u001b[32m   1463\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1464\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m9acedfa2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1465\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1466\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1467\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mYou can see that the accuracies across all the validation runs are far below that of the CNN test results above.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1468\u001b[39m    ]\n\u001b[32m   1469\u001b[39m   },\n\u001b[32m   1470\u001b[39m   {\n\u001b[32m   1471\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1472\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m8e27a131\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1473\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1474\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1475\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Task 5: Reflection\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1476\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1477\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms it! You\u001b[39m\u001b[33m'\u001b[39m\u001b[33mve reviewed the machine learning workflow. Before you go, let\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms reflect on a few things together to fill in the gaps!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1478\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1479\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1480\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 5-1: Reflecting on the Machine Learning Workflow\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1481\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1482\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1483\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGet together with your peer group. For the following tasks, you are expected to write a markdown cell describing the workflow required. You are free to include code, but **no Python code is required**. Discuss the following:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1484\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1485\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1. What would you need to do for your code if:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1486\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1487\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Your were to use your own data (for example, discuss survey data data and photos)?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1488\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- You were changing\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1489\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Your model?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1490\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Your scaling method?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1491\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m    - Your approach to handling missing data?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1492\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2. What is the significance of cross validation?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1493\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1494\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m### Further exploration\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1495\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1496\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn this exercise we only considered numerical data from the housing data - that is we left out the feature `ocean_proximity` which is not numerical. Find out about **One Hot Encoding** from Chapter 2 of the [Hands On Machine Learning book](https://eleanor.lib.gla.ac.uk/record=b4094676). Also watch the video on [Word Embedding and Word2Vec](https://www.youtube.com/watch?v=viZrOnJclY0), to get an intuition for **how textual content is transformed into numerical data**.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1497\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1498\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 5-2: Introducing the Tensorflow Playground\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1499\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1500\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBefore you go, let\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms play a little bit more with Neural Networks. There is an excellent online resource for this. Go to [playground.tensorflow.org](https://playground.tensorflow.org/). This site allows you to play around with different neural network architecture to see how well they perform in distinguishing data in different formation where data points of the same colour belong to the same class. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1501\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1502\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mChange your data type to \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33mspiral\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m by clicking on the picture for spiral data on the lefthand side. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1503\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- The idea is that the point of orange colour is one class and the ones of blue colour is another class. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1504\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- As the neural network learns you will see the image on the righthand side change background colour (blue/organge) - the class the neural network thinks the points in those regions belong to.  \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1505\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1506\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#### Task 5-2-1: Finding small networks that perform well.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1507\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1508\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Play around with the interface to get a feel for where everything is. For example, add more hidden layers (each layer is represented as nodes laid out vertically) and/or add nodes in any layer. Do this together in your group. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1509\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- Try to come up with the smallest network that will bring the training loss down to 0.2 or less. The traning loss is indicated on the right hand side - right underneath the label **Output**.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1510\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- In a Markdown cell below, describe how many layers with how many nodes you had in your network and how many epochs (indicated on the top lefthand corner) for your best model.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1511\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1512\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m#### Task 5-2-2: Examine the patterns displayed in the network nodes (see the image above). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1513\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1514\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDiscuss in your group and note down in a markdown cell: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1515\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m- what kinds of patterns the neural network might be learning at different layers and nodes. It is difficult to determine this for certain but you can get some intuition by hovering over the nodes in the tensorflow playground.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1516\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1517\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMarkdown cells have been included below for addressing the discussions in Task 5. This is for your convenience - modify as you see fit.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1518\u001b[39m    ]\n\u001b[32m   1519\u001b[39m   },\n\u001b[32m   1520\u001b[39m   {\n\u001b[32m   1521\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1522\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m94d01f6a\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1523\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1524\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1525\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m**Markdown cell for Task 5**\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1526\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1527\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1528\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m2.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1529\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m3. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1530\u001b[39m    ]\n\u001b[32m   1531\u001b[39m   },\n\u001b[32m   1532\u001b[39m   {\n\u001b[32m   1533\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1534\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m12a449b9\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1535\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1536\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1537\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m## Task 5-3 (Optional): Pre-trained Models\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1538\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1539\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mBefore we conclude this notebook, we will momentarily explore the **pre-trained model** VGG-19. This model was trained for computer vision and image classification. It was developed at Oxford but it is often considered to be the next generation model after AlexNet, which won the ImageNet challenge in 2012.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1540\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1541\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mThe model is introduced here to illustrate an example of a large convolutional neural network, much bigger than that used for MNIST classification task. Note how many more laters are involved, and the total parameters indicated at the bottom is huge. We can talk about this further if you should continue onto the course in Semester 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1542\u001b[39m    ]\n\u001b[32m   1543\u001b[39m   },\n\u001b[32m   1544\u001b[39m   {\n\u001b[32m   1545\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1546\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1547\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m5acc6f0c\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1548\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1549\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1550\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrom keras.applications.vgg19 import VGG19\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel = VGG19() ### this will take some time!!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1554\u001b[39m    ]\n\u001b[32m   1555\u001b[39m   },\n\u001b[32m   1556\u001b[39m   {\n\u001b[32m   1557\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1558\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mexecution_count\u001b[39m\u001b[33m\"\u001b[39m: null,\n\u001b[32m   1559\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m543b2700\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1560\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1561\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   1562\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1563\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprint(model.summary())\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1564\u001b[39m    ]\n\u001b[32m   1565\u001b[39m   },\n\u001b[32m   1566\u001b[39m   {\n\u001b[32m   1567\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcell_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmarkdown\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1568\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33ma7c37300\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1569\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {},\n\u001b[32m   1570\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   1571\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m# Summary\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1572\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1573\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mIn this notebook you learned about the machine learning pipeline. You reviewed the general workflow from class, and reflected on the workflow in the context of two example cases and data (housing data and minist data). You tried out **Linear Regression** and **Convolutional Neural Net Work**. You also briefly looked at something called a **Support Vector Machine** with **Stochastic Gradient Descent** (not covered in the lectures), comparing the performance for handwritten digit recognition. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1574\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1575\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAny one of these algorithms when looked at in detail, can be quite complex in terms of steps, as seen in the lectures and these labs. However, when using convenient libraries such as `sklearn`, many of them can be implemented in just a few lines. Having said that, where much of the complexity comes in is in preparing the data. And the **data needs more preparing when it is just collected from real world scenarios or sources**.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1576\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1577\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m**When data is curated** (such as the MNIST data), there is less to clean and prepare. However, if we are to discuss AI and bias, we need to to critically look at decisions made at the data curation stage. Often these decisions are not as transparent as it could be, which compromises our ability to assess the suitability of datasets, algorithms, and interpretation of results.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1578\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   1579\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWe also played with the Tensorflow Playground to enhance our intuition for neural networks. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1580\u001b[39m    ]\n\u001b[32m   1581\u001b[39m   }\n\u001b[32m   1582\u001b[39m  ],\n\u001b[32m   1583\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   1584\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mkernelspec\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   1585\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mdisplay_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mPython 3 (ipykernel)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1586\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1587\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpython3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1588\u001b[39m   },\n\u001b[32m   1589\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mlanguage_info\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   1590\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcodemirror_mode\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   1591\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mipython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1592\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m\n\u001b[32m   1593\u001b[39m    },\n\u001b[32m   1594\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mfile_extension\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1595\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mmimetype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext/x-python\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1596\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1597\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mnbconvert_exporter\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1598\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mpygments_lexer\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mipython3\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1599\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m3.9.18\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1600\u001b[39m   }\n\u001b[32m   1601\u001b[39m  },\n\u001b[32m   1602\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mnbformat\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m,\n\u001b[32m   1603\u001b[39m  \u001b[33m\"\u001b[39m\u001b[33mnbformat_minor\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5\u001b[39m\n\u001b[32m   1604\u001b[39m }\n",
      "\u001b[31mNameError\u001b[39m: name 'null' is not defined"
     ]
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"97c259f2\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Machine Learning by Example: from Start to End\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"d6539a1d\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Task 1: README!\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### The tasks in this notebook should be tackled in discussion with your peer group! \\n\",\n",
    "    \"\\n\",\n",
    "    \"**By asking and answering each other questions, you learn much more than doing independent work.** The article [“Embracing Digitalization: Student Learning and New Technologies” (Crittenden, Biel & Lovely 2018)](https://journals.sagepub.com/doi/10.1177/0273475318820895) shows how we learn and retain more information when we explain and discuss it with others.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Before you tackle a machine learning project you should make sure you keep the bigger picture in your head - this is your human input, skill, and imagination -  no AI can do this for you at the moment. The following sketches the typical steps involved in a machine learning project:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Step 1: Frame Your Problem\\n\",\n",
    "    \"    - What is the task? - Who will use it in what environment? What are the risks and impact?\\n\",\n",
    "    \"    - How will you measure performance of your model? Measures sufficient to assess potential risks and impacts?\\n\",\n",
    "    \"    - What are the assumptions? Document and review assumptions for bias. Question everything.\\n\",\n",
    "    \"- Step 2: Get Your Data\\n\",\n",
    "    \"    - Download your data - How will your get your data from where? Permissions and licenses? Suitable and reliable?\\n\",\n",
    "    \"    - Take a quick look at the data structure - how big is it? what fields/attributes are there and how many?\\n\",\n",
    "    \"    - Set aside test data - random split or stratified split? \\n\",\n",
    "    \"- Step 3: Prepare Your Data\\n\",\n",
    "    \"    - Handling Text/Categorical Data\\n\",\n",
    "    \"    - Scaling and Transformation\\n\",\n",
    "    \"    - Separate the labels from the rest of the attributes\\n\",\n",
    "    \"- Step 4: Select and Train Your Model\\n\",\n",
    "    \"    - train and evaluate on the training set\\n\",\n",
    "    \"    - cross-validation\\n\",\n",
    "    \"- Step 6: Test on Completely New Data\\n\",\n",
    "    \"- Step 7: Publish Your Results! Party! &#x1F389; &#x1F389; &#x1F389;\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, we will go through some of the key the steps. Some tasks will involve critical reflection, and others will be about coding. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Remember that, **if you are taking more than 30 minutes to do one task without any progress**, you should probably take a break. \\n\",\n",
    "    \"- Note down what you did and what errors you got in a markdown cell. This will help you understand the recurring errors, you will understand where you left off when you come back to it later, and also help you when you discuss the problem with your peers and with the lab tutors.  \\n\",\n",
    "    \"\\n\",\n",
    "    \"The code in this notebook is modified from that which was made available by Aurélien Géron and his fabulous book [\\\"Hands-On Machine Learning with Scitkit-Learn, Keras & Tensorflow\\\"](https://eleanor.lib.gla.ac.uk/record=b4094676).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"6025a3c8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 1-1: Checking Your Set Up\\n\",\n",
    "    \"\\n\",\n",
    "    \"It is important not only to check that you have the correct set of software and packages, but also that the version is the right one. If versions are not compatible with your code then it will throw up errors or unexpected results.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Python\\n\",\n",
    "    \"\\n\",\n",
    "    \"Check that your Python has version greater than 3.7 using the following code. This is what the code in the noteboook requires.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"eb2a627b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys # importing the package sys which lets you talk to your computer system.\\n\",\n",
    "    \"\\n\",\n",
    "    \"assert sys.version_info >= (3, 7) #versions are expressed a pair of numbers (3, 7) which is equivalent to 3.7. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2eb61a85\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The `assert` statement throws up an error when the statement following it is not true. If it is true, nothing will be shown. Experiment by replacing the numbers in the round brackets to be much bigger. **A Pair of numbers** like `(3, 7)` in round brackets is a data structure known as a **tuple** in programming lingo. \\n\",\n",
    "    \"\\n\",\n",
    "    \"### Scikit-Learn\\n\",\n",
    "    \"\\n\",\n",
    "    \"For this part you will need to have your environment installed with Python libraries `scikit-learn` and `packaging`. Review your Codespace exercise to rememebr how to install Python libraries. Note in the code below that when importing scikit-learn in the python code, you use `sklearn`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Check that your Scikit-Learn package version is greater than 1.0.1. In this case you will need to import `version` which is part of the `packaging` Python library. This allows you to extract/parse version numbers for Python packages/libraries like `sklearn`.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"c5882a1a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from packaging import version #import the package \\\"version\\\"\\n\",\n",
    "    \"import sklearn # import scikit-learn\\n\",\n",
    "    \"\\n\",\n",
    "    \"assert version.parse(sklearn.__version__) >= version.parse(\\\"1.0.1\\\") \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2dafd09c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 1-2: Review Machine Learning\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Step 1: Create a markdown cell to demonstrate your own reflection\\n\",\n",
    "    \"\\n\",\n",
    "    \"- In your markdown cell embed an image or link to a diagram illustrating the workflow from data to algorithm to model and data to model to predicted output. \\n\",\n",
    "    \"    - To embed images in your markdown cell, you can use the syntax `![alt text](image.jpg)` where you replace alt text with a description of your image (keep the square brackets!) and replace image.jpg with the file path and name of your image. \\n\",\n",
    "    \"    - To include a URL, use the syntax `[title](https://www.example.com)` where you replace title  with your own description, and https://www.example.com  with your own URL. Keep all brackets intact. \\n\",\n",
    "    \"    - For your reference, you can refer to the [markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/)  - note that HTML codes are also understood by your notebook.\\n\",\n",
    "    \"- Explain in your markdown cell how the examples in Lectures 3 & 4 align with the workflow. For example, what is the data, what was the learning algorithm, what is the model and what did the model output in response to new data?\\n\",\n",
    "    \"- In  your markdown cell, reflect on the range of ways to explain the workflow and the examples to a wider audience, for example, a museum curator?\\n\",\n",
    "    \"\\n\",\n",
    "    \"I've created a cell for you to use already below - double click on the area to start editing.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"72fdf563\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"***Markdown cell to modify***\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. \\n\",\n",
    "    \"2. \\n\",\n",
    "    \"3. \\n\",\n",
    "    \"4.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"d522ddc0-38c2-4eea-8dc5-227f9e6eee4a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 2: Discuss and report your reflection with your group\\n\",\n",
    "    \"- Get together with your peer group. Take turns to discuss your reflection above. If you have any difficulties, discuss these also.\\n\",\n",
    "    \"- Note down the results of your discussion in your notebook. In particular, note down anything that help you or others learn the topic. What approach could take in your notebook to engage the wider audience with your machine learning code.\\n\",\n",
    "    \"  \\n\",\n",
    "    \"I've created a cell for you to use already below - double click on the area to start editing.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"784b8573-0e7e-478a-a9b4-726386dc8b37\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"***Markdown cell to modify***\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. \\n\",\n",
    "    \"2. \\n\",\n",
    "    \"3. \\n\",\n",
    "    \"4.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"027e97d4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 1-3: Framing the Problem\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, we will be working with two datasets:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1)\\tTabular data consisting of information about houses in districts within the US state of California, and, \\n\",\n",
    "    \"2)\\tImage pixel data, each image representing a digit handwritten by high school students and employees of the US Census Bureau. \\n\",\n",
    "    \"\\n\",\n",
    "    \"The first of these datasets will be used to **predict median housing prices for a given district**. The results of the prediction will be combined with other data to determine whether it is worth investing in a given district. \\n\",\n",
    "    \"\\n\",\n",
    "    \"The second of these datasets will be used to **classify hand written digits**. It was originally developed as a way of sorting out the handwritten US zip codes (similar to UK postcodes) at the post office. \\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Step 1: Understand how framing the problem affects data selection\\n\",\n",
    "    \"\\n\",\n",
    "    \"- The academic article [“Rethinking the field of automatic prediction of court decisions”](https://link.springer.com/article/10.1007/s10506-021-09306-3) by Medvedeva, Wieling & Vols (2023), to appreciate how, depending on the objectives, the characteristics of data and algorithm might differ. \\n\",\n",
    "    \"- Read the BBC article [“AI facial recognition: Campaigners and MPs call for ban”](https://www.bbc.co.uk/news/technology-67022005) to understand that the same data, depending on its use, can raise concern about AI. We will be discussing prediction court decisions further in Week 7.\\n\",\n",
    "    \"- In view of the above, write down your reflection on the importance of framing your problem precisely in a notebook markdown cell - not only to define the task properly, but to understand how your machine learning model will be used down the road. \\n\",\n",
    "    \"\\n\",\n",
    "    \"### Step 2: How to select your algorithm\\n\",\n",
    "    \"\\n\",\n",
    "    \"In Lecture 2, we discussed how machine learning can be divided into three types: Supervised, Unsupervised, and Reinforcement. Large part of this course is focused on supervised learning – in particular, in this notebook, we will explore this using examples of regression and classification.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**To refresh your memory, read this short article from Codecademy** – [“Regression vs Classification”](https://www.codecademy.com/article/regression-vs-classification).\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Discuss with your peer group whether regression or classification would fit better for predicting median housing prices.\\n\",\n",
    "    \"- Discuss with your peer group whether regression or classification would fit better for handwritten digit recognition.\\n\",\n",
    "    \"- Write down the results of the discussion. In particular, report on what you concluded after the discussion and why.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Step 3: Before Data Collection\\n\",\n",
    "    \"\\n\",\n",
    "    \"Once your problem is defined (e.g. predicting the median housing price of a district), you will need to collect a new data set appropriate for your task, and/or identify existing data sets that can be used for training your model. \\n\",\n",
    "    \"\\n\",\n",
    "    \"- Discuss with your peer group what kind of information about housing in a district you think would help predict the median housing price in the district.\\n\",\n",
    "    \"- Discuss how these decisions might depend on geographical and/or cultural differences and how the information you collect would already bias the data. \\n\",\n",
    "    \"\\n\",\n",
    "    \"**Note these down the results of the discussions in Steps 2 & 3 in a markdown cell below.** I have already created a markdown cell for your use - just double click the area to begin editing.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0e3715aa\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"***Markdown cell to modify for Steps 1, 2 and 3***\\n\",\n",
    "    \"\\n\",\n",
    "    \"1.\\n\",\n",
    "    \"\\n\",\n",
    "    \"2.\\n\",\n",
    "    \"\\n\",\n",
    "    \"3.\\n\",\n",
    "    \"\\n\",\n",
    "    \"4.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4c9ae4ec\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Working with Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Overviews of machine learning and AI often make it seem as though the largest part of machine learning is in training the algorithm. This is misleading. In fact, [Forbes reported that about 80% of data science is related to data preparation](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/). This includes, among other things, data cleaning, re-scaling, and labelling.\\n\",\n",
    "    \"\\n\",\n",
    "    \"If you also include things like keeping track of what you did and why, storing and backing up data generated as well as the data used at the beginning, and recording the evaluation results, data exploration, interpretation, I would say that **95%** of machine learning is involved in **data management**. This can, in fact, be said to be a substantial part of achieving transparency, a corner stone of addressing the ethical concerns regarding AI and bias, fairness, data protection, explainability etc.\\n\",\n",
    "    \"\\n\",\n",
    "    \"So, let's get some data!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c15b4f64\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Task 2: Getting Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"Before anything else, you must get the data! There many ways you can get data. The scikit-learn's `datasets` package has some datasets already available to you. You can also get data from a nuumber of places such as OECD, OpenML, Kaggle, individual repositories in GitHub. \\n\",\n",
    "    \"\\n\",\n",
    "    \"In reality, data is everywhere. In fact, artists who make use of AI often make their own datasets: for example, check out [Anna Ridler's shell images](https://annaridler.com/the-shell-record-2021), [Caroline Sinders' feminist dataset](https://carolinesinders.com/feminist-data-set/), and [Refik Anadol's coral images](https://refikanadol.com/works-old/artificial-realities-coral/). While these may be owned by the artists, it can inspire new ways of thinking about data.\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this task we will look at something a little less artistic! &#x1F609;\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Example Tabular Data**: dataset comprising housing prices in California in the the United States. This dataset is available on the GitHub, courtesy of Aurelien Geron. \\n\",\n",
    "    \"- **Example Image Data**: MNIST dataset comprising images of handwritten digits. Handwritten digit recognition with the MNIST dataset is sometimes called the **\\\"Hello World!\\\" of machine learning**. \\n\",\n",
    "    \"\\n\",\n",
    "    \"We will use these datasets to carry out the prediction of housing prices and classification of digit images.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"656eba8c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 2-1: Download the Data: Example Tabular Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"The following code defines **function** called `load_housing_data()`. This function retrieves a compressed file avaialable at `https://github.com/ageron/data/raw/main/housing.tgz` and saves it in a folder `datasets` which is in the same folder as this notebook. This will be created if it does not exist. The code will also extract the contents in the folder `datasets`. It will then return the content of the data file `housing.csv` in the folder as a Pandas dataframe.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Pandas is a powerful and popular open-source data analysis and manipulation library for the Python programming language. It provides data structures and functions needed to work with structured data, such as tabulated data, seamlessly. Here are some key features:\\r\\n\",\n",
    "    \"\\r\\n\",\n",
    "    \"- DataFrame: A 2-dimensional labeled data structure with columns of potentially different types, similar to a table in a database or an Excel spreadsheet.\\r\\n\",\n",
    "    \"- Series: A 1-dimensional labeled array capable of holding any data type.\\r\\n\",\n",
    "    \"- Data Cleaning: Tools for handling missing data, filtering, and transforming data.\\r\\n\",\n",
    "    \"- Data Wrangling: Functions for merging, joining, and reshaping datasets.\\r\\n\",\n",
    "    \"- Time Series: Capabilities for working with time-series data, including date range generation and frequency conversion.\\r\\n\",\n",
    "    \"\\r\\n\",\n",
    "    \"Pandas is widely used in data science, machine learning, and data analysis projects due to its ease of use and flexibility. ing it?\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"56ec696a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import tarfile\\n\",\n",
    "    \"import urllib.request\\n\",\n",
    "    \"\\n\",\n",
    "    \"def load_housing_data(): #defines a function that loads the housing data available as .tgz file on a github URL\\n\",\n",
    "    \"    tarball_path = Path(\\\"datasets/housing.tgz\\\") # where you will save your compressed data\\n\",\n",
    "    \"    if not tarball_path.is_file():\\n\",\n",
    "    \"        Path(\\\"datasets\\\").mkdir(parents=True, exist_ok=True) #create datasets folder if it does not exist\\n\",\n",
    "    \"        url = \\\"https://github.com/ageron/data/raw/main/housing.tgz\\\" # url of where you are getting your data from\\n\",\n",
    "    \"        urllib.request.urlretrieve(url, tarball_path) # gets the url content and saves it at location specified by tarball_path\\n\",\n",
    "    \"        with tarfile.open(tarball_path) as housing_tarball: # opens saved compressed file as housing_tarball\\n\",\n",
    "    \"            housing_tarball.extractall(path=\\\"datasets\\\") # extracts the compressed content to datasets folder\\n\",\n",
    "    \"    return pd.read_csv(Path(\\\"datasets/housing/housing.csv\\\")) #uses panadas to read the csv file from the extracted content\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing = load_housing_data() #runsthe function defined above\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e678b056\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### If you've already downloaded and extracted the compressed file - then the following is all you need\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"741051dd\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing = pd.read_csv(Path(\\\"datasets/housing/housing.csv\\\"))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"fc852dd0\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Take a Quick Look: housing data\\n\",\n",
    "    \"\\n\",\n",
    "    \"With Pandas, you can get a summary of the data by using the method `info()`.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"e780f081\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing.info()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"bb46ae03\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The result above tells you how many attributes (e.g. longitude, latitude) characterise the dataset. How many are there? The data type float64 is a numerical data type. So, the table above also tells you how many attributes are not numerical. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"86c0ad40\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing[\\\"ocean_proximity\\\"].value_counts() # tells you what values the column for `ocean_proximity` can take\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"fc4b5110\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing.hist(bins=50, figsize=(12, 8))\\n\",\n",
    "    \"save_fig(\\\"attribute_histogram_plots\\\")  # extra code\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"5900bf86\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Finally you can run `housing.describe()` to get a summay of the data set `housing`.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"c3bc88c1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing.describe()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"24805732\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"At this point you should stop looking at the data until you have set aside test data. This is to prevent inadvertent bias creeping into the machine learning process.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"ae81bc17\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 2-2: Download the Data: Example Image Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"In contrast to tabular data, image data sets are not always read in using pandas. Technically you can do this (as the line below commented out suggests) but as there are no features human-friendly features (such as, median income etc.) - only pixel information, it does not always help to load it as a pandas dataframe, unless the model requires it to be so. Use the command `type` to see what data type `mnist` is - you can see that it is not a `pandas.core.frame.DataFrame`. Dataframes are not the preferred **data structure** for image data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"ae8a9ac0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.datasets import fetch_openml\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\\n\",\n",
    "    \"\\n\",\n",
    "    \"#mnist_dataframe = pd.DataFrame(data=mnist.data, columns=mnist.feature_names)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2311961e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Take a Quick Look: MNIST\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"fe3e0e2e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The command `mnist.info()` will not work here, to get information about the dataset content, because it is not a pandas dataframe. However, for datasets in the `sklearn.datasets` universe, you can use the keyword `DECSR` - as demonstrated in the first code cell below. The `print` command can be used in conjunction to get some useful context of the dataset structue and origin. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"f269f8bf\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(mnist.DESCR)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"3c410e32\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 2-3: Review the data description above with your group.\\n\",\n",
    "    \"\\n\",\n",
    "    \"What is the size of each image?\\n\",\n",
    "    \"\\n\",\n",
    "    \"Examine how LeCunn, Cortes, and Burges reorganised the NIST data as MNIST. Note that they remixed the data in two ways to create different a training dataset and test dataset. What they did do? Why do you think they did this? Was it justified? \\n\",\n",
    "    \"\\n\",\n",
    "    \"Write down the results in a markdown cell below. I have already created a markdown cell below - just double click to edit the content.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"55896634\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"***Mark Down cell for critique***\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. \\n\",\n",
    "    \"2.\\n\",\n",
    "    \"3.\\n\",\n",
    "    \"4.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"f2d5fba0\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### To see a full list of keys other than `DESCR` which is available to this dataset You can use the command `mnist.keys()` to see more keys available - run the code below.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"23a1d7e8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"mnist.keys()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"ed70b9bc\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 2-4: Identifying the Dimension of Images\\n\",\n",
    "    \"\\n\",\n",
    "    \"You may recognise some of the keys listed above for `mnist.keys()`. The keys `data` and `target` will return the image pixel data, and the labels (that is, the categories or classification) assigned to each of these images, respectively. \\n\",\n",
    "    \"\\n\",\n",
    "    \"- Create a code cell below to use these keys in Python, to use the `shape` command to verify the number of images in the dataset (70000) and how many features (e.g. pixels) represents the image (784). Print out the shape and the target categories. \\n\",\n",
    "    \"\\n\",\n",
    "    \"I have created a cell for you below with the data and target assigned to the **variables** `images` and `categories`. Add a line to print out the shape of the images and the list a assigned categories. Single click on the area to start editing. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"1ae51ee3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# cell for python code \\n\",\n",
    "    \"\\n\",\n",
    "    \"images = mnist.data\\n\",\n",
    "    \"categories = mnist.target\\n\",\n",
    "    \"\\n\",\n",
    "    \"# insert lines below to print the shape of images and to print the categories.\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(images.shape)\\n\",\n",
    "    \"print(categories)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7a8f61e6\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Let's take a look at one of the digits in the dataset - the first item.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"eb52d34a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"#extra code to visualise the image of digits\\n\",\n",
    "    \"\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"\\n\",\n",
    "    \"## the code below defines a function plot_digit. The initial key work `def` stands for define, followed by function name.\\n\",\n",
    "    \"## the function take one argument image_data in a parenthesis. This is followed by a colon. \\n\",\n",
    "    \"## Each line below that will be executed when the function is used. \\n\",\n",
    "    \"## This cell only defines the function. The next cell uses the function.\\n\",\n",
    "    \"\\n\",\n",
    "    \"def plot_digit(image_data): # defines a function so that you need not type all the lines below everytime you view an image\\n\",\n",
    "    \"    image = image_data.reshape(28, 28) #reshapes the data into a 28 x 28 image - before it was a string of 784 numbers\\n\",\n",
    "    \"    plt.imshow(image, cmap=\\\"binary\\\") # show the image in black and white - binary.\\n\",\n",
    "    \"    plt.axis(\\\"off\\\") # ensures no x and y axes are displayed\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"6f02abd3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# visualise a selected digit with the following code\\n\",\n",
    "    \"\\n\",\n",
    "    \"some_digit = mnist.data[0]\\n\",\n",
    "    \"plot_digit(some_digit)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"65e690ce\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Task 3: Setting Aside the Test Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"To set aside test data, you need to take shuffled and stratified samples. \\n\",\n",
    "    \"\\n\",\n",
    "    \"## Why Do We Shuffle\\n\",\n",
    "    \"\\n\",\n",
    "    \"The dataset you are working with could be ordered in a specific way (for example, all the data points in a specific class all at the top). If you select a percentage of 20% from the top, you could get data points in only specific classes. By shuffling we can avoid this. As it happens `sklearn` has a nifty function to allow you to split the data inclusive of splitting. This function is called `train_test_split`. See it in action below, using the housing data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"43c0ba19\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"\\n\",\n",
    "    \"tratio = 0.2 #to get 20% for testing and 80% for training\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_set, test_set = train_test_split(housing, test_size=tratio, random_state=42) \\n\",\n",
    "    \"## assigning a number to random_state means that everytime you run this you get the same split, unless you change the data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"6040547c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Why Do We Stratify\\n\",\n",
    "    \"\\n\",\n",
    "    \"If the dataset is skewed so that it contains more samples of a specific kind more than others, sampling randomly will result in your test data not representing the population you would like to test. An example of the estimated probability of getting a bad sample that does not reflect the actual population is provided below. The US population ratio of females in the census is 51.1%. The following is the probability of getting a sample with less than 48.5% or greater than 53.5% females if you take a random sample withoput stratifying: approximately **10.71%** \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"40e27820\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# extra code – shows another way to estimate the probability of bad sample\\n\",\n",
    "    \"\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"sample_size = 1000\\n\",\n",
    "    \"ratio_female = 0.511\\n\",\n",
    "    \"\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\\n\",\n",
    "    \"((samples < 485) | (samples > 535)).mean()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"38b4372c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 3.1: Stratified Sample: Housing Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"The following code adds a column to the `housing` data to create bins of data according to interval brackets of median income of districts. This is a first step to creating a stratified sample across different income brackets.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"75b0cd11\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing[\\\"income_cat\\\"] = pd.cut(housing[\\\"median_income\\\"],\\n\",\n",
    "    \"                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\\n\",\n",
    "    \"                               labels=[1, 2, 3, 4, 5])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"1ed7fbfe\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The following code uses the above bins to implement startified sampling - that is, it will randomly sample 20% (because we set test ratio `tratio` to 0.2) from each income bracket defined above.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"c69cc591\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"\\n\",\n",
    "    \"tratio = 0.2 #to get 20% for testing and 80% for training\\n\",\n",
    "    \"\\n\",\n",
    "    \"strat_train_set, strat_test_set = train_test_split(housing, test_size=tratio, stratify=housing[\\\"income_cat\\\"], random_state=42)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"19d7fcdc\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The code below prints out the proportion of each income category in the stratified test set above.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"57103983\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"strat_test_set[\\\"income_cat\\\"].value_counts() / len(strat_test_set) #Prints out in order of the highest proportion first.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"b0b1d994\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Note the attribute `random_state`. Setting this to a specific number like 42 **keeps the split the same everytime you run the code**. Keep in mind that it will not stay the same if you change the underlying dataset (e.g. adding more). \\n\",\n",
    "    \"\\n\",\n",
    "    \"Discuss with your peer group, why a stratified sample based on median income is reasonable. Create a markdown cell below to report on the results of the discussion. I have already create one below, so just double click to edit.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"6718cca3\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"***Markdown cell***\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. \\n\",\n",
    "    \"2. \\n\",\n",
    "    \"3.\\n\",\n",
    "    \"4.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d55a6a4f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"type(mnist.data)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"270dbdb6\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 3.2: Setting Aside Test Set for Image Data \\n\",\n",
    "    \"\\n\",\n",
    "    \"In the case of `mnist` the data is already cleaned prepared, scaled and ordered, so that the training data is the first 60,000 images, followed by the test data which is the last 10,000 images. So you need not shuffle and stratify nor use `train_test_split`. Instead, you can use the following code to set aside your test dataset. The data type of `mnist.data` is `numpy.ndarray` (you can verify this with the command `type`). \\n\",\n",
    "    \"- By using a colon and then 60000 in a square bracket after `mnist.data`, you are telling the computer that you want all the items up until the 60000th item (not including the 60000th) in the array `mnist.data`. We assign this to the **variable** `X_train`. \\n\",\n",
    "    \"- Likewise, the first 60000 categories corresponding the the first 60000 images are assigned to the **variable** `y_train`. \\n\",\n",
    "    \"- By using a colon after 60000, you are telling the computer you would like all the items from the 60000th onwards.\\n\",\n",
    "    \"\\n\",\n",
    "    \"It is machine learning convention to use upper case `X` for variable names associated with data and lower case `y` in the variable names associated to labels (or categories/classes).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"3ab94357\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train = mnist.data[:60000]\\n\",\n",
    "    \"y_train = mnist.target[:60000]\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_test = mnist.data[60000:]\\n\",\n",
    "    \"y_test = mnist.target[60000:]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"4fc35af8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Task 4: Selecting and Training a Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"You are finally ready to select and train your model. In the following code, we will use linear **regression for the prediction of district housing prices**, and a **convolutional neural network** for classification of hand written digits. For linear regression, we will use `Scikit-Learn`. For the convolutional neural networks we will use the `tensorflow` library with `keras`. Regardless of the model, the general flow is similar:  \\n\",\n",
    "    \"\\n\",\n",
    "    \"- Import the model from the relevant library. \\n\",\n",
    "    \"- Create an untrained model instance. \\n\",\n",
    "    \"- Fit the model to your training data.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Task 4-1: Housing Data and Linear Regression\\n\",\n",
    "    \"\\n\",\n",
    "    \"With linear regression, you need data, whose values are continuous - not discrete values such as categories. Note that it is not enough for the values to be numbers, which can also be categories (for example, your place in a queue is a number but is never a fraction like 1.33). The feature `income_cat` is another category expressed as a number. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Before doing anything else, let's assign a copy of the stratified training set we created earlier to the variable `housing`. You should always work with copies of data and never look at the test set in case we inadvertently use information in the test to improve the performance (**data snooping bias**).\\n\",\n",
    "    \"\\n\",\n",
    "    \"To do this use the following code.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"9b6cbdb3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing = strat_train_set.copy()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"90f59211\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 1: Checking Correlations: Training Set\\n\",\n",
    "    \"\\n\",\n",
    "    \"Linear regression in essence works by picking up on correlations between features. So it can be useful to explore the correlations especially between the target value you are trying to predict `median_house_value` and the other features in the dataset, e.g. `median_income`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The training set we have is of type `pandas.DataFrame`.  For pandas, dataframes have the function `corr` which calculates the correlations for you. The code is below - first it calculates all the correlations between all the pairs of features and saves it in variable `corr_matrix`. \\n\",\n",
    "    \"\\n\",\n",
    "    \"We can take a look at correlations for `median_house_value` by using that feature name in a square bracket (**with quotation marks!**). the last part `sort_values(ascending=False)` sorts the correlation to display it in descending order of correlation (that is, most correlated fetures are listed first).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"4767a968\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"corr_matrix = housing.corr(numeric_only=True) # argument is so that it only calculates for numeric value features\\n\",\n",
    "    \"corr_matrix[\\\"median_house_value\\\"].sort_values(ascending=False)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e741e0d8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 2: Visualise the Correlations\\n\",\n",
    "    \"\\n\",\n",
    "    \"Pandas also can visualise these correlations as a graph for you. In the code below, we have selected four features (see the variable with that name), to 4 x 4 grid of graphs.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"1b7facae\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from pandas.plotting import scatter_matrix\\n\",\n",
    "    \"\\n\",\n",
    "    \"features = [\\\"median_house_value\\\", \\\"median_income\\\", \\\"total_rooms\\\",\\n\",\n",
    "    \"              \\\"housing_median_age\\\"]\\n\",\n",
    "    \"scatter_matrix(housing[features], figsize=(12, 8))\\n\",\n",
    "    \"#save_fig(\\\"scatter_matrix_plot\\\")  \\n\",\n",
    "    \"\\n\",\n",
    "    \"#The line above is extra code you can uncomment (remove the hash at the begining) to save the image.\\n\",\n",
    "    \"#But, to use this, make sure you ran the code at the beginning of this notebook defining the save_fig function\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"94c4e919\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 3: Separate the Target Labels from Your Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"In any machine learning task, you need to provide the data and the target Label separately to the machine learning algorithm. Otherwise, they have no way of knowing which of the features is the target label. In our scenario, the label for the housing data is the `median_house_value`. When your data is in a padas dataframe format, you can simply 1) drop the column with the label to get the data, and, 2) get the column for the target label, to get the labels.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"610bec83\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing = strat_train_set.drop(\\\"median_house_value\\\", axis=1) ## 1)\\n\",\n",
    "    \"housing_labels = strat_train_set[\\\"median_house_value\\\"].copy() ## 2)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"57a9e4b6\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 4: Look for Missing Values in the Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"When working with tabular data, it is quite common to find that some rows are missing values for some of the columns. If you run the `info` command for dataframes (we've done this in [Task 2-1](#Task-2-1:-Download-the-Data:-Example-Tabular-Data) above!). \\n\",\n",
    "    \"\\n\",\n",
    "    \"- Running the code will show the total number of entries. By comparing that number to the number of Non-Null entries for each feature (e.g. `total_bedrooms`) you can see whether there are missing values. \\n\",\n",
    "    \"- If there are no missing values, these numbers should be the same!\\n\",\n",
    "    \"\\n\",\n",
    "    \"How many values are missing for the number of `total_bedrooms'? \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"3bffc540\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing.info()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"31181096\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 5: Handling Missing Values\\n\",\n",
    "    \"\\n\",\n",
    "    \"To handle the missing values, you need a code in place to tell the machine what to do if there are missing values. In-depth discussion of handling missing values is beyond the scope of this course, but there are three common ways of handing these:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- (Option 1) Drop the row with missing value. This causes you to lose data points. In our scenario with the housing data, 168 rows will be removed.\\n\",\n",
    "    \"- (Option 2) Drop the column with missing values. This causes you to lose one of your features.\\n\",\n",
    "    \"- (Option 3) Fill in the missing value with some value such as the median or mean or fixed value that makes sense. This is called **imputing**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Depending on which approach you take, the performance of your AI could be different. Also, note that, **with Option 1, you will have to remove the corresponding rows in `housing_labels` before using these in a machine learning task**. Following are codes from each of these approaches. Read the comments included in the code for understanding what each cell is doing.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"b0ee6f1d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# this is the code for Option 1 above. \\n\",\n",
    "    \"housing_option1 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing_option1.dropna(subset=[\\\"total_bedrooms\\\"], inplace=True)  # option 1 - dropping the rows where total_bedroom is missing values.\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing_option1.info() #look for missing values after rows have been dropped\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"e4e11d2f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing_option2 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing_option2.drop(\\\"total_bedrooms\\\", axis=1, inplace=True)  # option 2 - dropping the column associated with total_bedrooms\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing_option2.info() # checking for missing values in the new data after column has been dropped\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"74dba58c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing_option3 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\\n\",\n",
    "    \"\\n\",\n",
    "    \"median = housing[\\\"total_bedrooms\\\"].median() # calculating mean of the value for total_bedrooms to use in filling missing values\\n\",\n",
    "    \"housing_option3[\\\"total_bedrooms\\\"].fillna(median, inplace=True)  # option 3 - filling missing values with the median\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing_option3.info()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0cf9695a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### You can also use `SimpleImputer` from the `sklearn.impute` library to fill missing values with the median\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"2b28f434\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.impute import SimpleImputer\\n\",\n",
    "    \"\\n\",\n",
    "    \"imputer = SimpleImputer(strategy=\\\"median\\\") # initialises the imputer\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing_num = housing.select_dtypes(include=[np.number]) ## includes only numeric features in the data\\n\",\n",
    "    \"\\n\",\n",
    "    \"imputer.fit(housing_num) #calculates the median for each numeric feature so that the imputer can use them\\n\",\n",
    "    \"\\n\",\n",
    "    \"housing_num[:] = imputer.transform(housing_num) # the imputer uses the median to fill the missing values and saves the result in variable X\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"32c1a0b1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 6: Scaling Your Features\\n\",\n",
    "    \"\\n\",\n",
    "    \"Machine learning algorithms learn better when similar scales are used across all the features. For example, the numeric range of values for `total_rooms` will be totally different from `median_income`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Test this with he **min** and **max** values after running the pandas `describe()` function. Code below.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"ead8cda8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing_num.describe()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"fb924d5b\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"You can see that all the features have very different ranges. Bringing these in alignment is called **feature scaling**. There are a number of ways to scale features. Scikit-Learn provides something called MinMaxScaler which scales the values to fit into a range defined by you. Below, the code is provided for when you are fitting it into the range from -1 to 1. AI algorithms often like the mean to be placed at zero, so best to set a range with zero as the mid point value. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"396bcce8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.preprocessing import MinMaxScaler # get the MinMaxScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"min_max_scaler = MinMaxScaler(feature_range=(-1, 1)) # setup an instance of a scaler\\n\",\n",
    "    \"housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)# use the scaler to transform the data housing_num\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"afc23108\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"Alternatively, Scikit-Learn also provides a method called StandardScaler. This method tries normalise the distributional characteristics by considering mean and standard deviation for each feature, and normalising the values to have standard deviation 1. But, even without knowing the mathematical details, we can simply employ the tools provided by `sklearn` - example below.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d3eb2e06\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"std_scaler = StandardScaler()\\n\",\n",
    "    \"housing_num_std_scaled = std_scaler.fit_transform(housing_num)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"43bd704e\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"housing_num[:]=std_scaler.fit_transform(housing_num)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"1018f320\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 7: Train a Linear Regression Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"In the first instance we will use the data resulting from the `SimpleImputer` (with the **median** as a strategy) and use   `StandardScaler` for scaling the features. Before we train the Linear Regression model for predicting median housing prices for districts, we need to also apply the scaling to the target labels (in our case, the known median housing prices). The code is provided below.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"483b52a3\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.preprocessing import StandardScaler #This line is not necessary if you ran this prior to running this cell. \\n\",\n",
    "    \"#We are however including it here for completeness sake.\\n\",\n",
    "    \"\\n\",\n",
    "    \"target_scaler = StandardScaler() #instance of Scaler\\n\",\n",
    "    \"scaled_labels = target_scaler.fit_transform(housing_labels.to_frame()) #calculate the mean and standard deviation and use it to transform the target labels.\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"2ed4b20f\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Training Step\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"fdc4e78d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.linear_model import LinearRegression #get the library from sklearn.linear model\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = LinearRegression() #get an instance of the untrained model\\n\",\n",
    "    \"model.fit(housing_num, scaled_labels)\\n\",\n",
    "    \"#model.fit(housing[[\\\"median_income\\\"]], scaled_labels) #fit it to your data\\n\",\n",
    "    \"#some_new_data = housing[[\\\"median_income\\\"]].iloc[:5]  # pretend this is new data\\n\",\n",
    "    \"\\n\",\n",
    "    \"#scaled_predictions = model.predict(some_new_data)\\n\",\n",
    "    \"#predictions = target_scaler.inverse_transform(scaled_predictions)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"129a1e49\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"some_new_data = housing_num.iloc[:5] #pretend this is new data\\n\",\n",
    "    \"#some_new_data = housing[[\\\"median_income\\\"]].iloc[:5]  # pretend this is new data\\n\",\n",
    "    \"\\n\",\n",
    "    \"scaled_predictions = model.predict(some_new_data)\\n\",\n",
    "    \"predictions = target_scaler.inverse_transform(scaled_predictions)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"ebd4851b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(predictions, housing_labels.iloc[:5])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"3980d05c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# extra code – computes the error ratios discussed in the book\\n\",\n",
    "    \"error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1\\n\",\n",
    "    \"print(\\\", \\\".join([f\\\"{100 * ratio:.1f}%\\\" for ratio in error_ratios]))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"f01ebd5a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 8: Cross Validation\\n\",\n",
    "    \"\\n\",\n",
    "    \"As mentioned in Lecture 4 - pre-recorded lecture - having one training set and one test set to check performance is limited in producing a robust AI model. What you really want to see is a stable performance across many training sets and test sets. IN the first stance you want to test the model on the training set.\\n\",\n",
    "    \"\\n\",\n",
    "    \"One way to evaluate your model before testing on the new data (the data you set aside as your test data) is cross validation. This where you split your training data into many pieces, then leave on of the pieces out for testing.The code below does that!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"cf15d382\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.model_selection import cross_val_score\\n\",\n",
    "    \"\\n\",\n",
    "    \"rmses = -cross_val_score(model, housing_num, scaled_labels,\\n\",\n",
    "    \"                              scoring=\\\"neg_root_mean_squared_error\\\", cv=10)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"30ea58a8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"pd.Series(rmses).describe()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c936dc9a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 4-2: Hand Written Digit Classification\\n\",\n",
    "    \"\\n\",\n",
    "    \"As mentioned earlier, as an example, for the hand written digits, we will use a specific kind of neural network model called a **Convolutional Neural Network (CNN)** model. In the lectures, we learned about general neural networks but not CNN. If you want to get a feel for CNNs, you can watch the Stat Quest Video [Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)](https://www.youtube.com/watch?v=HGwBXDKFk9I). If you feel confident to go deeper, Chapters 19 and 22 of Russell and Norvig's Book [\\\"Artificial Intelligence: a modern approach\\\"](https://eleanor.lib.gla.ac.uk/record=b3897063) is an excellent read, not to mention Géron's book [\\\"Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow\\\"](https://eleanor.lib.gla.ac.uk/record=b4094676).\\n\",\n",
    "    \"\\n\",\n",
    "    \"For this task, **we will move away from `sklearn` and use `tensorflow` and `keras` instead**. Tensorflow and Keras are popular libraries recognised for their usefulness in building neural networks quickly. Although we already loaded the data from `sklearn`, in the code below, we will get it again from `tensorflow.keras.datasets`. This will allow you to experience getting data from another library and also make it easier to work with the subsequent code because everything will happen with tensorflow. \\n\",\n",
    "    \"\\n\",\n",
    "    \"The data is already fairly organised, so, the data cleaning part of the operation can be abbreviated. This is however not a characteristic of image data. It is a characteristic of **curated data** which is not the same as real world messy data (such as the housing data from earlier). \\n\",\n",
    "    \"\\n\",\n",
    "    \"The code for importing the libraries and getting the data has been included below. To get these to work, **you will need to have your environment installed with `tensorflow` and `keras`**. In the first line of the first code cell below, you will notice that `tensorflow` is imported as `tf`. This is a recognised convention in the machine learning community. Adopting this convention makes your code more readable for this community. Once you have imported the library that way, `tf` will be used subsequently instead of `tensorflow`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Step 1: Get the Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"542b6908\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"\\n\",\n",
    "    \"mnist = tf.keras.datasets.mnist.load_data()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"8b89621d\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 2: Review What the Data Looks Like  \\n\",\n",
    "    \"\\n\",\n",
    "    \"You can review information about what this dataset looks like at the Keras page for the [MNIST digits classification dataset](https://keras.io/api/datasets/mnist/). The page makes it clear that `mnist` above is organised as a data type called **tuple** - something that looks like `(a,b)`. The `a` and `b` are tuples themselves, representing training and test data, respectively. Check first that mnist is a **tuple** with following line of code.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"45cdcbee\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(type(mnist))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"1adf9270\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The **Keras webpages are useful** for looking up and getting information about wide range of keras commands you might encouter in machine learning programs. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"1805f5f4\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 3: How to Get the Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"To get the data out of `a` and `b`, run the following code. Read the comment for explanation. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"c621149f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"(X_train_full, y_train_full), (X_test, y_test) = mnist \\n\",\n",
    "    \"# (X_train_full, y_train_full) is the 'tuple' related to `a` and (X_test, y_test) is the 'tuple' related to `b`.\\n\",\n",
    "    \"# X_train_full is the full training data and y_train_full are the corresponding labels \\n\",\n",
    "    \"# - labels indicate what digit the image is of, for example 5 if it is an image of a handwritten 5.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"9a62b91a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 4: Scaling the Pixel Values (the features)\\n\",\n",
    "    \"\\n\",\n",
    "    \"In dealing with images, there are four main comsiderations that most frequently arise: \\n\",\n",
    "    \"- 1) input size of the image (height and width in terms of pixels)\\n\",\n",
    "    \"- 2) whether you want to move the pixels so that the image is centered in the middle\\n\",\n",
    "    \"- 3) scaling the value of the pixels to be in a specified range. \\n\",\n",
    "    \"\\n\",\n",
    "    \"The neural network we will use will works best with pixel values between 0 and 1. Pixels in a black and white image usually have values between 0 and 255. The code below simply rescales these, dividing by 255. There are other ways of scaling this, similar to when we scaled the feature values of the `housing` data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"7c876f2c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train_full = X_train_full / 255.\\n\",\n",
    "    \"X_test = X_test / 255.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"bf244fc8\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 5: Split the Training Data into Training and Validation Data\\n\",\n",
    "    \"\\n\",\n",
    "    \"We already have data split into training and test data. The **validation data is split from the training data** and used to evaluate the performance during training. This is **different from test data** which is completely new data not seen during training or fine tuning. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Test data is used for the final test before publishing the results. In fact in many competitions, the test data is **withheld behind an application interface** so that contestants cannot engage in **data snooping**. \\n\",\n",
    "    \"\\n\",\n",
    "    \"The code below takes the last 5000 images for validation data. The second line does the same for the corresponding labels.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"9197eb2f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\\n\",\n",
    "    \"y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"d405ef72\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 5: Increasing Dimension to Include Colour Channels\\n\",\n",
    "    \"\\n\",\n",
    "    \"An image is usually represented as a (width x height) block of pixels. When presenting your images to the neural network, you need to add an extra dimension to your image representation, to indicate the number of colour channels your images are using. Normally, for a greyscale image this would be 1, while for a RBG colour image it would be 3. \\n\",\n",
    "    \"\\n\",\n",
    "    \"All in all you will be submitting something that has shape like `(N, W, H, C)` where `N` is number of images, `W` is the width of any one image, `H` is the height of any one image, and `C` is the number channels (1 for greyscale, 3 for colour). \\n\",\n",
    "    \"\\n\",\n",
    "    \"All your images are expected to be the same size as it enters the neural network. \\n\",\n",
    "    \"\\n\",\n",
    "    \"The mnist dataset currently has a shape like `(N, W, H)`. Your numpy library allows you to add the required extra dimension. The code below does this.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"eb5d4722\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import numpy as np # you won't need to run this line if you ran it before in this notebook. But for completeness.\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_train = X_train[..., np.newaxis] #adds a dimension to the image training set - the three dots means keeping everything else the same.\\n\",\n",
    "    \"X_valid = X_valid[..., np.newaxis]\\n\",\n",
    "    \"X_test = X_test[..., np.newaxis]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"6f9f3aba\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 6: Build the Neural Network and Fit it to the Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a2a69e1c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"tf.keras.backend.clear_session()\\n\",\n",
    "    \"\\n\",\n",
    "    \"tf.random.set_seed(42)\\n\",\n",
    "    \"np.random.seed(42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Unlike scikit-learn, with tensorflow and keras, the model is built by defining each layer of the neural network.\\n\",\n",
    "    \"# Below, everytime tf.keras.layers is called it is building in another layer\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = tf.keras.Sequential([\\n\",\n",
    "    \"    tf.keras.layers.Conv2D(32, kernel_size=3, padding=\\\"same\\\", activation=\\\"relu\\\", kernel_initializer=\\\"he_normal\\\"),\\n\",\n",
    "    \"    tf.keras.layers.Conv2D(64, kernel_size=3, padding=\\\"same\\\", activation=\\\"relu\\\", kernel_initializer=\\\"he_normal\\\"),\\n\",\n",
    "    \"    tf.keras.layers.MaxPool2D(),\\n\",\n",
    "    \"    tf.keras.layers.Flatten(),\\n\",\n",
    "    \"    tf.keras.layers.Dropout(0.25),\\n\",\n",
    "    \"    tf.keras.layers.Dense(128, activation=\\\"relu\\\", kernel_initializer=\\\"he_normal\\\"),\\n\",\n",
    "    \"    tf.keras.layers.Dropout(0.5),\\n\",\n",
    "    \"    tf.keras.layers.Dense(10, activation=\\\"softmax\\\")\\n\",\n",
    "    \"])\\n\",\n",
    "    \"model.compile(loss=\\\"sparse_categorical_crossentropy\\\", optimizer=\\\"nadam\\\", \\n\",\n",
    "    \"              metrics=[\\\"accuracy\\\"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"d356c6f5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"model.summary() # not necessary for the machine learning task.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"022105d9\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"The summary above is not easy to read initially but it is a presentation of each layer. The numbers at the bottom tell you how many parameters need learning in this model. The visualisation can be useful later when you get more used to neural networks if you should continue on to Semester 2.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"0e414d05\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Step 7: Train and Evaluate the Model \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"f36a2f27\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"\\n\",\n",
    "    \"model.evaluate(X_test, y_test)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"dfc4b587\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Comparing with Another Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Below you are provided with code for using something called **Stochastic Gradient Decent Classifier**. This model applies the stochastic gradient descent optimiser (cf. the **nadam** optimiser used with the CNN above) with any number of algorithms but by default it applies it to a **Support Vector Machine**. \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"568ed072\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# getting the data again from Scikit-Learn, so that we know the image dimens fit for the model!\\n\",\n",
    "    \"\\n\",\n",
    "    \"from sklearn.datasets import fetch_openml\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# getting the data and the categories for the data\\n\",\n",
    "    \"images = mnist.data\\n\",\n",
    "    \"categories = mnist.target\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"1185aac1\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"**Normally, we would set aside the test data**. \\n\",\n",
    "    \"\\n\",\n",
    "    \"But in this experiement we will abbreviate and use the entire data and evaluate using cross validation, especially since we are not intending, on this occasion, to develop our model with the validation step. **Note that running this might take a while - so be patient!**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"5a621d01\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from sklearn.linear_model import SGDClassifier\\n\",\n",
    "    \"from sklearn.model_selection import cross_val_score\\n\",\n",
    "    \"\\n\",\n",
    "    \"sgd_clf = SGDClassifier(random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"#cross validation on training data for fit accuracy\\n\",\n",
    "    \"\\n\",\n",
    "    \"accuracy = cross_val_score(sgd_clf, images, categories, cv=10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(accuracy)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"9acedfa2\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"You can see that the accuracies across all the validation runs are far below that of the CNN test results above.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"8e27a131\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Task 5: Reflection\\n\",\n",
    "    \"\\n\",\n",
    "    \"That's it! You've reviewed the machine learning workflow. Before you go, let's reflect on a few things together to fill in the gaps!\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Task 5-1: Reflecting on the Machine Learning Workflow\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"Get together with your peer group. For the following tasks, you are expected to write a markdown cell describing the workflow required. You are free to include code, but **no Python code is required**. Discuss the following:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. What would you need to do for your code if:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Your were to use your own data (for example, discuss survey data data and photos)?\\n\",\n",
    "    \"- You were changing\\n\",\n",
    "    \"    - Your model?\\n\",\n",
    "    \"    - Your scaling method?\\n\",\n",
    "    \"    - Your approach to handling missing data?\\n\",\n",
    "    \"2. What is the significance of cross validation?\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Further exploration\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this exercise we only considered numerical data from the housing data - that is we left out the feature `ocean_proximity` which is not numerical. Find out about **One Hot Encoding** from Chapter 2 of the [Hands On Machine Learning book](https://eleanor.lib.gla.ac.uk/record=b4094676). Also watch the video on [Word Embedding and Word2Vec](https://www.youtube.com/watch?v=viZrOnJclY0), to get an intuition for **how textual content is transformed into numerical data**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Task 5-2: Introducing the Tensorflow Playground\\n\",\n",
    "    \"\\n\",\n",
    "    \"Before you go, let's play a little bit more with Neural Networks. There is an excellent online resource for this. Go to [playground.tensorflow.org](https://playground.tensorflow.org/). This site allows you to play around with different neural network architecture to see how well they perform in distinguishing data in different formation where data points of the same colour belong to the same class. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Change your data type to \\\"spiral\\\" by clicking on the picture for spiral data on the lefthand side. \\n\",\n",
    "    \"- The idea is that the point of orange colour is one class and the ones of blue colour is another class. \\n\",\n",
    "    \"- As the neural network learns you will see the image on the righthand side change background colour (blue/organge) - the class the neural network thinks the points in those regions belong to.  \\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Task 5-2-1: Finding small networks that perform well.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Play around with the interface to get a feel for where everything is. For example, add more hidden layers (each layer is represented as nodes laid out vertically) and/or add nodes in any layer. Do this together in your group. \\n\",\n",
    "    \"- Try to come up with the smallest network that will bring the training loss down to 0.2 or less. The traning loss is indicated on the right hand side - right underneath the label **Output**.\\n\",\n",
    "    \"- In a Markdown cell below, describe how many layers with how many nodes you had in your network and how many epochs (indicated on the top lefthand corner) for your best model.\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### Task 5-2-2: Examine the patterns displayed in the network nodes (see the image above). \\n\",\n",
    "    \"\\n\",\n",
    "    \"Discuss in your group and note down in a markdown cell: \\n\",\n",
    "    \"- what kinds of patterns the neural network might be learning at different layers and nodes. It is difficult to determine this for certain but you can get some intuition by hovering over the nodes in the tensorflow playground.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Markdown cells have been included below for addressing the discussions in Task 5. This is for your convenience - modify as you see fit.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"94d01f6a\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"**Markdown cell for Task 5**\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. \\n\",\n",
    "    \"2.\\n\",\n",
    "    \"3. \\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"12a449b9\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Task 5-3 (Optional): Pre-trained Models\\n\",\n",
    "    \"\\n\",\n",
    "    \"Before we conclude this notebook, we will momentarily explore the **pre-trained model** VGG-19. This model was trained for computer vision and image classification. It was developed at Oxford but it is often considered to be the next generation model after AlexNet, which won the ImageNet challenge in 2012.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The model is introduced here to illustrate an example of a large convolutional neural network, much bigger than that used for MNIST classification task. Note how many more laters are involved, and the total parameters indicated at the bottom is huge. We can talk about this further if you should continue onto the course in Semester 2.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"5acc6f0c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from keras.applications.vgg19 import VGG19\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = VGG19() ### this will take some time!!\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"543b2700\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"print(model.summary())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"a7c37300\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook you learned about the machine learning pipeline. You reviewed the general workflow from class, and reflected on the workflow in the context of two example cases and data (housing data and minist data). You tried out **Linear Regression** and **Convolutional Neural Net Work**. You also briefly looked at something called a **Support Vector Machine** with **Stochastic Gradient Descent** (not covered in the lectures), comparing the performance for handwritten digit recognition. \\n\",\n",
    "    \"\\n\",\n",
    "    \"Any one of these algorithms when looked at in detail, can be quite complex in terms of steps, as seen in the lectures and these labs. However, when using convenient libraries such as `sklearn`, many of them can be implemented in just a few lines. Having said that, where much of the complexity comes in is in preparing the data. And the **data needs more preparing when it is just collected from real world scenarios or sources**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**When data is curated** (such as the MNIST data), there is less to clean and prepare. However, if we are to discuss AI and bias, we need to to critically look at decisions made at the data curation stage. Often these decisions are not as transparent as it could be, which compromises our ability to assess the suitability of datasets, algorithms, and interpretation of results.\\n\",\n",
    "    \"\\n\",\n",
    "    \"We also played with the Tensorflow Playground to enhance our intuition for neural networks. \"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.18\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
